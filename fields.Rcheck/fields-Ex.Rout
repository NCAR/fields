
R version 4.0.5 (2021-03-31) -- "Shake and Throw"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin17.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "fields"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> base::assign(".ExTimings", "fields-Ex.timings", pos = 'CheckExEnv')
> base::cat("name\tuser\tsystem\telapsed\n", file=base::get(".ExTimings", pos = 'CheckExEnv'))
> base::assign(".format_ptime",
+ function(x) {
+   if(!is.na(x[4L])) x[1L] <- x[1L] + x[4L]
+   if(!is.na(x[5L])) x[2L] <- x[2L] + x[5L]
+   options(OutDec = '.')
+   format(x[1L:3L], digits = 7L)
+ },
+ pos = 'CheckExEnv')
> 
> ### * </HEADER>
> library('fields')
Loading required package: spam
Loading required package: dotCall64
Loading required package: grid
Spam version 2.5-1 (2019-12-12) is loaded.
Type 'help( Spam)' or 'demo( spam)' for a short introduction 
and overview of this package.
Help for individual functions is also obtained by adding the
suffix '.spam' to the function name, e.g. 'help( chol.spam)'.

Attaching package: ‘spam’

The following objects are masked from ‘package:base’:

    backsolve, forwardsolve

Loading required package: viridis
Loading required package: viridisLite
See https://github.com/NCAR/Fields for
 an extensive vignette, other supplements and source code 
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("BD")
> ### * BD
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: BD
> ### Title: Data frame of the effect of buffer compositions on DNA strand
> ###   displacement amplification. A 4-d regression data set with with
> ###   replication. This is a useful test data set for exercising function
> ###   fitting methods.
> ### Aliases: BD
> ### Keywords: datasets
> 
> ### ** Examples
> 
> # fitting a DNA strand 
> # displacement amplification  surface to various buffer compositions 
> fit<- Tps(BD[,1:4],BD$lnya,scale.type="range") 
> surface(fit)  # plots fitted surface and contours 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("BD", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("CO")
> ### * CO
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Colorado Monthly Meteorological Data
> ### Title: Monthly surface meterology for Colorado 1895-1997
> ### Aliases: COmonthlyMet CO.elev CO.id CO.loc CO.names CO.ppt CO.ppt.MAM
> ###   CO.tmax CO.tmax.MAM CO.tmin CO.tmin.MAM CO.years CO.ppt.MAM.climate
> ###   CO.tmax.MAM.climate CO.tmean.MAM.climate CO.tmin.MAM.climate
> ###   CO.elevGrid CO.Grid
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
> data(COmonthlyMet)
> 
> #Spatial plot of 1997 Spring average daily maximum temps
>  quilt.plot( CO.loc,CO.tmax.MAM[103,]  )
>  US( add=TRUE)
>  title( "Recorded MAM max temperatures (1997)")
> 
> # min and max temperatures against elevation
> 
> matplot( CO.elev, cbind( CO.tmax.MAM[103,], CO.tmin.MAM[103,]),
+   pch="o", type="p",
+   col=c("red", "blue"), xlab="Elevation (m)", ylab="Temperature (C)")
> title("Recorded MAM max (red) and min (blue) temperatures 1997")
> 
> #Fitting a spatial model:
> obj<- Tps(CO.loc,CO.tmax.MAM.climate,  Z= CO.elev )
> ## Not run: 
> ##D out<- spatialProcess(CO.loc,CO.tmax.MAM.climate, 
> ##D           smoothness=1.0, Z= CO.elev)
> ##D surface( out)
> ##D 	  
> ## End(Not run)
> 	  
>          
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("CO", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("CO2")
> ### * CO2
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: CO2
> ### Title: Simulated global CO2 observations
> ### Aliases: CO2 CO2.true
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D data(CO2)
> ##D #
> ##D # A quick look at the observations with world map
> ##D quilt.plot( CO2$lon.lat, CO2$y)
> ##D world( add=TRUE)
> ##D 
> ##D # Note high concentrations in Borneo (biomass burning), Amazonia and
> ##D # ... Michigan (???).
> ##D 
> ##D # spatial smoothing using the wendland compactly supported covariance
> ##D # see help( fastTps) for details
> ##D # First smooth using locations and Euclidean distances 
> ##D # note taper is in units of degrees 
> ##D out<-fastTps( CO2$lon.lat, CO2$y, aRange=4, lambda=2.0) 
> ##D #summary of fit note about 7300 degrees of freedom 
> ##D # associated with fitted surface
> ##D  print( out)
> ##D # image plot on a grid  (this takes a while)
> ##D surface( out, type="I", nx=300, ny=150)
> ##D # smooth with respect to great circle distance 
> ##D out2<-fastTps( CO2$lon.lat, CO2$y, lon.lat=TRUE,lambda=1.5, aRange=4*68) 
> ##D print(out2)
> ##D #surface( out2, type="I", nx=300, ny=150)
> ##D 
> ##D # these data are actually subsampled from a grid. 
> ##D # create the image object that holds the data
> ##D #
> ##D 
> ##D temp<- matrix( NA, ncol=ncol(CO2.true$z), nrow=nrow(CO2.true$z))
> ##D temp[ CO2.true$mask] <- CO2$y
> ##D 
> ##D # look at gridded object. 
> ##D  image.plot(CO2.true$x,CO2.true$y, temp)
> ##D 
> ##D # to predict _exactly_ on this grid for the second fit;
> ##D # (this take a while)
> ##D look<- predictSurface( out2, grid.list=list( x=CO2.true$x, y=CO2.true$y))
> ##D image.plot(look)
> ##D 
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("CO2", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("CovarianceUpper")
> ### * CovarianceUpper
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: CovarianceUpper
> ### Title: Evaluate covariance over upper triangle of distance matrix
> ### Aliases: ExponentialUpper
> ### Keywords: covariance
> 
> ### ** Examples
> 
> set.seed(123)
> 
> #make distance matrix using the random locations
> coords = matrix(runif(10), ncol=2)
> distMat = rdist(coords)
> 
> #compute covariance matrix, but only over the upper triangle
> upperCov = ExponentialUpper(distMat, range=.1)
> 
> print(distMat)
          [,1]       [,2]      [,3]       [,4]      [,5]
[1,] 0.0000000 0.69540037 0.8555197 0.78132050 0.7715140
[2,] 0.6954004 0.00000000 0.5259413 0.09754322 0.1681197
[3,] 0.8555197 0.52594131 0.0000000 0.58393877 0.6873190
[4,] 0.7813205 0.09754322 0.5839388 0.00000000 0.1108665
[5,] 0.7715140 0.16811974 0.6873190 0.11086647 0.0000000
> print(upperCov)
     [,1]         [,2]         [,3]        [,4]         [,5]
[1,]    1 0.0009548048 0.0001925418 0.000404360 0.0004460228
[2,]    0 1.0000000000 0.0051983548 0.377029350 0.1861509438
[3,]    0 0.0000000000 1.0000000000 0.002910624 0.0010351697
[4,]    0 0.0000000000 0.0000000000 1.000000000 0.3299993164
[5,]    0 0.0000000000 0.0000000000 0.000000000 1.0000000000
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("CovarianceUpper", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("Exponential")
> ### * Exponential
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Exponential, Matern, Radial Basis
> ### Title: Covariance functions
> ### Aliases: Exponential Matern Matern.cor.to.range RadialBasis
> ### Keywords: spatial
> 
> ### ** Examples
> 
> # a Matern correlation function 
>  d<- seq( 0,10,,200)
>  y<- Matern( d, range=1.5, smoothness=1.0)
>  plot( d,y, type="l")
> 
> # Several Materns of different smoothness with a similar correlation 
> # range
> 
> # find ranges for nu = .5, 1.0 and 2.0 
> # where the correlation drops to .1 at a distance of 10 units.
> 
>  r1<- Matern.cor.to.range( 10, nu=.5, cor.target=.1)
>  r2<- Matern.cor.to.range( 10, nu=1.0, cor.target=.1)
>  r3<- Matern.cor.to.range( 10, nu=2.0, cor.target=.1)
> 
> # note that these equivalent ranges
> # with respect to this correlation length are quite different
> # due the different smoothness parameters. 
> 
>  d<- seq( 0, 15,,200)
>  y<- cbind(  Matern( d, range=r1, nu=.5),
+              Matern( d, range=r2, nu=1.0),
+              Matern( d, range=r3, nu=2.0))
> 
>  matplot( d, y, type="l", lty=1, lwd=2)
>  xline( 10)
>  yline( .1)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("Exponential", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("Krig.Amatrix")
> ### * Krig.Amatrix
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Krig.Amatrix
> ### Title: Smoother (or "hat") matrix relating predicted values to the
> ###   dependent (Y) values.
> ### Aliases: Krig.Amatrix
> ### Keywords: spatial
> 
> ### ** Examples
> 
> # Compute the A matrix or "hat" matrix for a thin plate spline 
> # check that this gives the same predicted values  
> tps.out<-Tps( ChicagoO3$x, ChicagoO3$y)
> A<-Krig.Amatrix( tps.out, ChicagoO3$x)
> test<- A%*%ChicagoO3$y 
> # now compare this to predict( tps.out) or tps.out$fitted.values 
> #                    they should be the same 
> stats( test- tps.out$fitted.values)
                        [,1]
N               2.000000e+01
mean           -1.421085e-14
Std.Dev.        7.290015e-15
min            -2.842171e-14
Q1             -2.131628e-14
median         -1.421085e-14
Q3             -1.243450e-14
max             0.000000e+00
missing values  0.000000e+00
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("Krig.Amatrix", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("Krig")
> ### * Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Krig
> ### Title: Kriging surface estimate
> ### Aliases: Krig resid.Krig fitted.Krig coef.Krig
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> # a 2-d example 
> # fitting a surface to ozone  
> # measurements. Exponential covariance, range parameter is 20 (in miles) 
> 
> fit <- Krig(ChicagoO3$x, ChicagoO3$y, aRange=20)  
>  
> summary( fit) # summary of fit 
CALL:
Krig(x = ChicagoO3$x, Y = ChicagoO3$y, aRange = 20)
                                             
 Number of Observations:                20   
 Number of unique points:               20   
 Number of parameters in the null space 3    
 Parameters for fixed spatial drift     3    
 Effective degrees of freedom:          5.4  
 Residual degrees of freedom:           14.6 
 MLE tau                                3.733
 GCV tau                                4.049
 MLE sigma                              4.796
 Scale passed for covariance (sigma)    <NA> 
 Scale passed for nugget (tau^2)        <NA> 
 Smoothing parameter lambda             2.906

Residual Summary:
    min   1st Q  median   3rd Q     max 
-6.4820 -1.6170 -0.5672  1.8010  7.4870 

Covariance Model: stationary.cov
  Covariance function is 
  Names of non-default covariance arguments: 
       aRange

DETAILS ON SMOOTHING PARAMETER:
 Method used:   REML    Cost:  1
   lambda       trA       GCV   GCV.one GCV.model    tauHat 
    2.906     5.394    22.452    22.452        NA     4.049 

 Summary of all estimates found for lambda
           lambda   trA   GCV tauHat -lnLike Prof converge
GCV         4.120 4.799 22.39  4.125        49.26        4
GCV.model      NA    NA    NA     NA           NA       NA
GCV.one     4.120 4.799 22.39  4.125           NA        4
RMSE           NA    NA    NA     NA           NA       NA
pure error     NA    NA    NA     NA           NA       NA
REML        2.906 5.394 22.45  4.049        49.24        2
> set.panel( 2,2) 
plot window will lay out plots in a 2 by 2 matrix 
> plot(fit) # four diagnostic plots of fit  
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> surface( fit, type="C") # look at the surface 
> 
> # predict at data
> predict( fit)
          [,1]
 [1,] 38.32080
 [2,] 38.83593
 [3,] 38.12672
 [4,] 38.76760
 [5,] 39.04168
 [6,] 39.90186
 [7,] 38.56459
 [8,] 39.53299
 [9,] 39.89821
[10,] 39.65490
[11,] 40.75214
[12,] 40.19162
[13,] 40.92551
[14,] 40.57972
[15,] 41.04454
[16,] 40.43037
[17,] 39.62459
[18,] 39.41161
[19,] 41.03544
[20,] 40.96869
> 
> # predict using 7.5 effective degrees of freedom:
> predict( fit, df=7.5)
          [,1]
 [1,] 37.49035
 [2,] 38.19014
 [3,] 37.08185
 [4,] 38.27208
 [5,] 38.57329
 [6,] 40.11896
 [7,] 37.86026
 [8,] 39.26797
 [9,] 40.17369
[10,] 39.51753
[11,] 41.19223
[12,] 40.22450
[13,] 41.24735
[14,] 40.62566
[15,] 41.53740
[16,] 41.16773
[17,] 39.48732
[18,] 39.64519
[19,] 41.95582
[20,] 41.98016
> 
> 
> # predict on a grid ( grid chosen here by defaults)
>  out<- predictSurface( fit)
>  surface( out, type="C") # option "C" our favorite
> 
> # predict at arbitrary points (10,-10) and (20, 15)
>  xnew<- rbind( c( 10, -10), c( 20, 15))
>  predict( fit, xnew)
         [,1]
[1,] 38.28184
[2,] 40.13606
> 
> # standard errors of prediction based on covariance model.  
>  predictSE( fit, xnew)
[1] 1.632105 3.054689
> 
> # surface of standard errors on a default grid
>  predictSurfaceSE( fit)-> out.p # this takes some time!
>  surface( out.p, type="C")
>  points( fit$x)
> 
> ## Not run: 
> ##D # Using another stationary covariance. 
> ##D # smoothness is the shape parameter for the Matern. 
> ##D 
> ##D fit <- Krig(ChicagoO3$x, ChicagoO3$y,
> ##D Covariance="Matern", aRange=10, smoothness=1.0)  
> ##D summary( fit)
> ##D 
> ##D #
> ##D # Roll your own: creating very simple user defined Gaussian covariance 
> ##D #
> ##D 
> ##D test.cov <- function(x1,x2,aRange,marginal=FALSE,C=NA){
> ##D    # return marginal variance
> ##D      if( marginal) { return(rep( 1, nrow( x1)))}
> ##D 
> ##D     # find cross covariance matrix     
> ##D       temp<- exp(-(rdist(x1,x2)/aRange)**2)
> ##D       if( is.na(C[1])){
> ##D           return( temp)}
> ##D       else{
> ##D           return( temp%*%C)}
> ##D       } 
> ##D #
> ##D # use this and put in quadratic polynomial fixed function 
> ##D 
> ##D 
> ##D  fit.flame<- Krig(flame$x, flame$y, cov.function="test.cov", m=3, aRange=.5)
> ##D 
> ##D #
> ##D # note how range parameter is passed to Krig.   
> ##D # BTW:  GCV indicates an interpolating model (nugget variance is zero) 
> ##D # This is the content of the warning message.
> ##D 
> ##D # take a look ...
> ##D  surface(fit.flame, type="I") 
> ## End(Not run)
> 
> # 
> # Thin plate spline fit to ozone data using the radial 
> # basis function as a generalized covariance function 
> #
> # p=2 is the power in the radial basis function (with a log term added for 
> # even dimensions)
> # If m is the degree of derivative in penalty then p=2m-d 
> # where d is the dimension of x. p must be greater than 0. 
> #  In the example below p = 2*2 - 2 = 2  
> #
> 
>  out<- Krig( ChicagoO3$x, ChicagoO3$y,cov.function="Rad.cov", 
+                        m=2,p=2,scale.type="range") 
> 
> # See also the Fields function Tps
> # out  should be identical to  Tps( ChicagoO3$x, ChicagoO3$y)
> # 
> 
> ## Not run: 
> ##D #
> ##D #
> ##D # explore some different values for the range and lambda using GCV
> ##D   data(ozone2)
> ##D   aRange <- seq(200,600,,40)
> ##D   GCV<- matrix( NA, 40,80)
> ##D # the loop 
> ##D   for( k in 1:40){
> ##D # call to Krig with different ranges
> ##D # also turn off warnings for GCV search 
> ##D # to avoid lots of messages. (not recommended in general!)
> ##D     obj<-Krig( ozone2$lon.lat,ozone2$y[16,],
> ##D              cov.function="stationary.cov", 
> ##D              aRange=aRange[k],
> ##D              Covariance="Matern",smoothness=1.0, 
> ##D              Distance="rdist.earth", nstep.cv=80,
> ##D              give.warnings=FALSE, na.rm=TRUE)
> ##D     GCV[k,]<-obj$gcv.grid[,3]
> ##D   }
> ##D # get lambda grid  from looping 
> ##D    k<- 1
> ##D    lam<-  Krig( ozone2$lon.lat,ozone2$y[16,],
> ##D              cov.function="stationary.cov", 
> ##D              aRange=aRange[k], 
> ##D              Covariance="Matern",smoothness=.5, 
> ##D              Distance="rdist.earth", nstep.cv=80,
> ##D              give.warnings=FALSE, na.rm=TRUE)$gcv.grid[,1]
> ##D  matplot( log10(lam), t(GCV),type="l",lty=1)
> ##D  
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("Krig.engine.default")
> ### * Krig.engine.default
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: The Engines:
> ### Title: Basic linear algebra utilities and other computations supporting
> ###   the Krig function.
> ### Aliases: Krig.engine.default Krig.engine.fixed Krig.coef Krig.check.xY
> ###   Krig.transform.xY Krig.make.u Krig.make.W Krig.make.Wi %d*%
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> Krig( ChicagoO3$x, ChicagoO3$y, aRange=100)-> out
> 
> Krig.engine.default( out)-> stuff
> 
> # compare "stuff" to components in out$matrices
> 
> look1<- Krig.coef( out)
> look1$c
            [,1]
 [1,] -2.4887193
 [2,] -5.9442552
 [3,] -9.4559993
 [4,] -6.3077137
 [5,] -1.7111851
 [6,]  0.3705694
 [7,] -1.9656335
 [8,] -1.5278066
 [9,]  6.4522473
[10,] -1.7351620
[11,]  1.9239144
[12,] -0.2111423
[13,]  1.5976487
[14,] -1.5625771
[15,]  2.2762835
[16,]  5.3098945
[17,] -6.8591942
[18,] 11.2403805
[19,]  2.3703084
[20,]  8.2281417
> # compare to out$c
> 
> look2<- Krig.coef( out, yM = ChicagoO3$y)
> look2$c
            [,1]
 [1,] -2.4887193
 [2,] -5.9442552
 [3,] -9.4559993
 [4,] -6.3077137
 [5,] -1.7111851
 [6,]  0.3705694
 [7,] -1.9656335
 [8,] -1.5278066
 [9,]  6.4522473
[10,] -1.7351620
[11,]  1.9239144
[12,] -0.2111423
[13,]  1.5976487
[14,] -1.5625771
[15,]  2.2762835
[16,]  5.3098945
[17,] -6.8591942
[18,] 11.2403805
[19,]  2.3703084
[20,]  8.2281417
> # better be the same even though we pass as new data!
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("Krig.engine.default", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("Krig.replicates")
> ### * Krig.replicates
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Krig.replicates
> ### Title: Collapse repeated spatial locations into unique locations
> ### Aliases: Krig.replicates
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> #create  some spatial replicates
>  set.seed( 123)
>  x0<- matrix( runif(10*2), 10,2)
>  x<-  x0[ c(rep(1,3), 2:8, rep( 9,5),10) , ]
>  y<-  rnorm( 16)
>  
>  out<- Krig.replicates( x=x, y=y)
> # compare 
> # out$yM[1] ;  mean( y[1:3])
> # out$yM[9] ; mean( y[11:15])
> # mean( y[ out$rep.info==9])
>  
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("Krig.replicates", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("KrigFindLambda")
> ### * KrigFindLambda
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: KrigFindLambda
> ### Title: Finds profile likelihood and GCV estimates of smoothing
> ###   parameters for splines and Kriging.
> ### Aliases: KrigFindLambda gcv.sreg
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> # 
> Tps( ChicagoO3$x, ChicagoO3$y)-> obj # default is to find lambda by GCV
> summary( obj)
CALL:
Tps(x = ChicagoO3$x, Y = ChicagoO3$y)
                                               
 Number of Observations:                20     
 Number of unique points:               20     
 Number of parameters in the null space 3      
 Parameters for fixed spatial drift     3      
 Effective degrees of freedom:          4.5    
 Residual degrees of freedom:           15.5   
 MLE tau                                3.779  
 GCV tau                                4.073  
 MLE sigma                              347.7  
 Scale passed for covariance (sigma)    <NA>   
 Scale passed for nugget (tau^2)        <NA>   
 Smoothing parameter lambda             0.04107

Residual Summary:
    min   1st Q  median   3rd Q     max 
-6.8060 -1.4390 -0.5064  1.4440  7.7890 

Covariance Model: Rad.cov
  Names of non-default covariance arguments: 
       p

DETAILS ON SMOOTHING PARAMETER:
 Method used:   GCV    Cost:  1
   lambda       trA       GCV   GCV.one GCV.model    tauHat 
  0.04107   4.50304  21.40938  21.40938        NA   4.07296 

 Summary of all estimates found for lambda
            lambda   trA   GCV tauHat -lnLike Prof converge
GCV        0.04107 4.503 21.41  4.073        49.00        5
GCV.model       NA    NA    NA     NA           NA       NA
GCV.one    0.04107 4.503 21.41  4.073           NA        5
RMSE            NA    NA    NA     NA           NA       NA
pure error      NA    NA    NA     NA           NA       NA
REML       0.02972 4.886 21.49  4.030        48.98        4
> 
> KrigFindLambda( obj)-> out
> print( out$lambda.est) # results agree with Tps summary
               lambda      trA      GCV   tauHat -lnLike Prof converge
GCV        0.04107313 4.503038 21.40938 4.072962     49.00042        5
GCV.model          NA       NA       NA       NA           NA       NA
GCV.one    0.04107313 4.503038 21.40938 4.072962           NA        5
RMSE               NA       NA       NA       NA           NA       NA
pure error         NA       NA       NA       NA           NA       NA
REML       0.02971891 4.886273 21.48837 4.029698     48.98323        4
> 
> sreg( rat.diet$t, rat.diet$trt)-> out
> gcv.sreg( out, tol=1e-10) # higher tolerance search for minimum 
$gcv.grid
         lambda       trA       GCV   GCV.one GCV.model    tauHat
1  8.183514e-04 37.049991  8.075493  8.075493        NA 0.6354341
2  1.028159e-03 36.645399  8.050161  8.050161        NA 0.6971539
3  1.291758e-03 36.177994  8.017769  8.017769        NA 0.7616816
4  1.622937e-03 35.645737  7.976059  7.976059        NA 0.8282481
5  2.039024e-03 35.048882  7.922064  7.922064        NA 0.8958739
6  2.561787e-03 34.390175  7.852044  7.852044        NA 0.9633881
7  3.218575e-03 33.674681  7.761566  7.761566        NA 1.0294736
8  4.043749e-03 32.909227  7.645807  7.645807        NA 1.0927368
9  5.080481e-03 32.101546  7.500139  7.500139        NA 1.1518031
10 6.383009e-03 31.259303  7.320957  7.320957        NA 1.2054290
11 8.019478e-03 30.389253  7.106600  7.106600        NA 1.2526192
12 1.007550e-02 29.496749  6.858138  6.858138        NA 1.2927272
13 1.265865e-02 28.585697  6.579728  6.579728        NA 1.3255215
14 1.590406e-02 27.658936  6.278397  6.278397        NA 1.3511980
15 1.998152e-02 26.718887  5.963220  5.963220        NA 1.3703358
16 2.510436e-02 25.768238  5.644109  5.644109        NA 1.3838029
17 3.154059e-02 24.810474  5.330515  5.330515        NA 1.3926315
18 3.962693e-02 23.850097  5.030364  5.030364        NA 1.3978879
19 4.978644e-02 22.892527  4.749417  4.749417        NA 1.4005594
20 6.255063e-02 21.943732  4.491090  4.491090        NA 1.4014757
21 7.858729e-02 21.009719  4.256659  4.256659        NA 1.4012676
22 9.873541e-02 20.096021  4.045690  4.045690        NA 1.4003629
23 1.240491e-01 19.207263  3.856567  3.856567        NA 1.3990108
24 1.558526e-01 18.346901  3.687000  3.687000        NA 1.3973233
25 1.958100e-01 17.517133  3.534459  3.534459        NA 1.3953247
26 2.460115e-01 16.718988  3.396497  3.396497        NA 1.3929989
27 3.090837e-01 15.952539  3.270965  3.270965        NA 1.3903276
28 3.883262e-01 15.217195  3.156109  3.156109        NA 1.3873155
29 4.878848e-01 14.511992  3.050601  3.050601        NA 1.3840034
30 6.129682e-01 13.835842  2.953499  2.953499        NA 1.3804710
31 7.701203e-01 13.187710  2.864191  2.864191        NA 1.3768351
32 9.675628e-01 12.566692  2.782335  2.782335        NA 1.3732454
33 1.215625e+00 11.972029  2.707798  2.707798        NA 1.3698802
34 1.527286e+00 11.403079  2.640610  2.640610        NA 1.3669421
35 1.918850e+00 10.859261  2.580903  2.580903        NA 1.3646500
36 2.410803e+00 10.340010  2.528864  2.528864        NA 1.3632278
37 3.028882e+00  9.844737  2.484663  2.484663        NA 1.3628873
38 3.805423e+00  9.372811  2.448394  2.448394        NA 1.3638090
39 4.781053e+00  8.923550  2.420010  2.420010        NA 1.3661223
40 6.006814e+00  8.496230  2.399287  2.399287        NA 1.3698897
41 7.546834e+00  8.090092  2.385799  2.385799        NA 1.3750976
42 9.481682e+00  7.704359  2.378923  2.378923        NA 1.3816557
43 1.191259e+01  7.338243  2.377862  2.377862        NA 1.3894041
44 1.496672e+01  6.990946  2.381688  2.381688        NA 1.3981267
45 1.880387e+01  6.661671  2.389389  2.389389        NA 1.4075698
46 2.362479e+01  6.349617  2.399931  2.399931        NA 1.4174613
47 2.968168e+01  6.053988  2.412314  2.412314        NA 1.4275325
48 3.729144e+01  5.774001  2.425636  2.425636        NA 1.4375387
49 4.685218e+01  5.508892  2.439160  2.439160        NA 1.4472801
50 5.886408e+01  5.257928  2.452380  2.452380        NA 1.4566240
51 7.395559e+01  5.020415  2.465096  2.465096        NA 1.4655265
52 9.291624e+01  4.795705  2.477490  2.477490        NA 1.4740559
53 1.167380e+02  4.583193  2.490207  2.490207        NA 1.4824182
54 1.466672e+02  4.382320  2.504464  2.504464        NA 1.4909878
55 1.842695e+02  4.192570  2.522173  2.522173        NA 1.5003448
56 2.315123e+02  4.013463  2.546106  2.546106        NA 1.5113200
57 2.908672e+02  3.844543  2.580099  2.580099        NA 1.5250435
58 3.654394e+02  3.685346  2.629251  2.629251        NA 1.5429833
59 4.591303e+02  3.535377  2.700100  2.700100        NA 1.5669507
60 5.768417e+02  3.394090  2.800645  2.800645        NA 1.5990342
61 7.247317e+02  3.260896  2.940108  2.940108        NA 1.6414254
62 9.105376e+02  3.135210  3.128290  3.128290        NA 1.6961152
63 1.143980e+03  3.016522  3.374422  3.374422        NA 1.7644891
64 1.437273e+03  2.904481  3.685565  3.685565        NA 1.8469127
65 1.805759e+03  2.798959  4.064815  4.064815        NA 1.9424445
66 2.268717e+03  2.700060  4.509777  4.509777        NA 2.0487936
67 2.850368e+03  2.608079  5.011838  5.011838        NA 2.1625627
68 3.581143e+03  2.523411  5.556621  5.556621        NA 2.2797131
69 4.499272e+03  2.446432  6.125644  6.125644        NA 2.3961195
70 5.652790e+03  2.377396  6.698779  6.698779        NA 2.5080728
71 7.102046e+03  2.316354  7.256855  7.256855        NA 2.6126316
72 8.922860e+03  2.263129  7.783785  7.783785        NA 2.7077851
73 1.121049e+04  2.217331  8.267813  8.267813        NA 2.7924451
74 1.408463e+04  2.178399  8.701848  8.701848        NA 2.8663206
75 1.769563e+04  2.145660  9.083044  9.083044        NA 2.9297309
76 2.223241e+04  2.118387  9.411967  9.411967        NA 2.9834094
77 2.793233e+04  2.095851  9.691596  9.691596        NA 3.0283282
78 3.509359e+04  2.077357  9.926394  9.926394        NA 3.0655601
79 4.409085e+04  2.062267 10.121545 10.121545        NA 3.0961801
80 5.539481e+04  2.050011 10.282391 10.282391        NA 3.1212022

$lambda.est
             lambda     trA      GCV   tauHat converge
GCV        11.10732 7.44848 2.377623 1.386913       16
GCV.model        NA      NA       NA       NA       NA
GCV.one    11.10732 7.44848 2.377623 1.386913       16
RMSE             NA      NA       NA       NA       NA
pure error       NA      NA       NA       NA       NA

$warningTable
           Warning Refine indexMIN leftEndpoint rightEndpoint   lambda    effdf
GCV          FALSE   TRUE       43        FALSE         FALSE 11.91259 7.338243
GCV.model    FALSE  FALSE       NA           NA            NA       NA       NA
GCV.one      FALSE   TRUE       43        FALSE         FALSE 11.91259 7.338243
RMSE         FALSE  FALSE       NA           NA            NA       NA       NA
pure error   FALSE  FALSE       NA           NA            NA       NA       NA

> ## Not run: 
> ##D # a simulation example
> ##D x<- seq( 0,1,,150)
> ##D f<-  x**2*( 1-x)
> ##D f<- f/sqrt( var( f))
> ##D 
> ##D set.seed(123) # let's all use the same seed
> ##D tau<- .1
> ##D y<- f + rnorm( 150)*tau
> ##D 
> ##D Tps( x,y)-> obj # create Krig object
> ##D 
> ##D hold<- hold2<- matrix( NA, ncol=6, nrow=200)
> ##D 
> ##D for( k in 1:200){
> ##D # look at GCV estimates of lambda
> ##D # new data simulated
> ##D    y<- f + rnorm(150)*tau 
> ##D # save GCV estimates
> ##D lambdaTable<- KrigFindLambda(obj,  y=y, give.warnings=FALSE)$lambda.est
> ##D hold[k,]<-  lambdaTable[1,]
> ##D hold2[k,]<-  lambdaTable[6,]
> ##D }
> ##D matplot( cbind(hold[,2], hold2[,2]),cbind( hold[,4],hold2[,4]),
> ##D  xlab="estimated eff. df", ylab="tau hat", pch=16, col=c("orange3", "green2"), type="p")
> ##D yline( tau, col="grey", lwd=2)
> ##D 
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("KrigFindLambda", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("NorthAmericanRainfall")
> ### * NorthAmericanRainfall
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: NorthAmericanRainfall
> ### Title: Observed North American summer precipitation from the historical
> ###   climate network.
> ### Aliases: NorthAmericanRainfall
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(NorthAmericanRainfall)
> x<- cbind(NorthAmericanRainfall$longitude,  NorthAmericanRainfall$latitude)
> y<- NorthAmericanRainfall$precip
> quilt.plot( x,y)
> world( add=TRUE)
> 
> Zstat<- NorthAmericanRainfall$trend / NorthAmericanRainfall$trendSE
> quilt.plot( x, Zstat)
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("NorthAmericanRainfall", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("QTps")
> ### * QTps
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: QTps
> ### Title: Robust and Quantile smoothing using a thin-plate spline
> ### Aliases: QSreg QTps
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> data(ozone2)
> x<- ozone2$lon.lat
> y<- ozone2$y[16,]
> 
> 
> 
> # Smoothing fixed at 50 df 
>     look1<- QTps( x,y, psi.scale= 15, df= 50)
Warning in QTps(x, y, psi.scale = 15, df = 50) :
  6 missing value(s) removed from data
> 
> ## Not run: 
> ##D # Least squares spline (because scale is so large)
> ##D     look2<- QTps( x,y, psi.scale= 100, df= 50)
> ##D #
> ##D     y.outlier<- y
> ##D # add in a huge outlier.
> ##D     y.outlier[58]<- 1e5
> ##D     look.outlier1<- QTps( x,y.outlier, psi.scale= 15, df= 50)
> ##D # least squares spline.
> ##D     look.outlier2<- QTps( x,y.outlier, psi.scale=100 , df= 50)
> ##D #
> ##D     set.panel(2,2)
> ##D     surface( look1)
> ##D     title("robust spline")
> ##D     surface( look2)
> ##D     title("least squares spline")
> ##D     surface( look.outlier1,  zlim=c(0,250))
> ##D     title("robust spline w/outlier") 
> ##D     points( rbind(x[58,]), pch="+")
> ##D     surface( look.outlier2, zlim=c(0,250))
> ##D     title("least squares spline w/outlier")
> ##D     points( rbind(x[58,]), pch="+")
> ##D     set.panel()
> ## End(Not run)
> # some quantiles
> look50 <- QTps( x,y, psi.scale=.5)
Warning in QTps(x, y, psi.scale = 0.5) :
  6 missing value(s) removed from data
> look75 <- QTps( x,y,f.start= look50$fitted.values, alpha=.75)
Warning in QTps(x, y, f.start = look50$fitted.values, alpha = 0.75) :
  6 missing value(s) removed from data
> 
> 
> # a simulated example that finds some different quantiles. 
> ## Not run: 
> ##D set.seed(123)
> ##D N<- 400
> ##D x<- matrix(runif( N), ncol=1)
> ##D true.g<- x *(1-x)*2
> ##D true.g<- true.g/ mean( abs( true.g))
> ##D y<-  true.g + .2*rnorm( N )
> ##D 
> ##D look0 <- QTps( x,y, psi.scale=10, df= 15)
> ##D look50 <- QTps( x,y, df=15)
> ##D look75 <- QTps( x,y,f.start= look50$fitted.values, df=15, alpha=.75)
> ## End(Not run)
> 
> ## Not run: 
> ##D # this example tests the quantile estimate by Monte Carlo
> ##D # by creating many replicate point to increase the sample size. 
> ##D # Replicate points are used because the computations for the 
> ##D # spline are dominated by the number of unique locations not the 
> ##D # total number of points. 
> ##D set.seed(123)
> ##D N<- 80
> ##D M<- 200
> ##D x<- matrix( sort(runif( N)), ncol=1)
> ##D x<- matrix( rep( x[,1],M), ncol=1)
> ##D 
> ##D true.g<- x *(1-x)*2
> ##D true.g<- true.g/ mean( abs( true.g))
> ##D errors<- .2*(rexp( N*M) -1)
> ##D y<- c(matrix(true.g, ncol=M, nrow=N) + .2 *  matrix( errors, ncol=M, nrow=N))
> ##D 
> ##D look0 <- QTps( x,y, psi.scale=10, df= 15)
> ##D look50 <- QTps( x,y, df=15)
> ##D look75 <- QTps( x,y, df=15, alpha=.75)
> ##D 
> ##D 
> ##D bplot.xy(x,y, N=25)
> ##D xg<- seq(0,1,,200)
> ##D lines( xg, predict( look0, x=xg), col="red")
> ##D lines( xg, predict( look50, x=xg), col="blue")
> ##D lines( xg, predict( look75, x=xg), col="green")
> ## End(Not run)
> ## Not run: 
> ##D # A comparison with qsreg
> ##D   qsreg.fit50<- qsreg(rat.diet$t,rat.diet$con, sc=.5)
> ##D   lam<- qsreg.fit50$cv.grid[,1]
> ##D   df<- qsreg.fit50$cv.grid[,2]
> ##D   M<- length(lam)
> ##D   CV<-rep( NA, M)
> ##D   M<- length( df)
> ##D   fhat.old<- NULL
> ##D   for ( k in M:1){
> ##D      temp.obj<- QTps(rat.diet$t,rat.diet$con, f.start=fhat.old,  psi.scale=.5, tolerance=1e-6,
> ##D      verbose=FALSE, df= df[k])
> ##D      cat(k, " ")
> ##D      CV[k] <- temp.obj$Qinfo$CV.psuedo
> ##D      fhat.old<- temp.obj$fitted.values
> ##D   }
> ##D   plot( df, CV, type="l", lwd=2)
> ##D # psuedo data estimate
> ##D   points( qsreg.fit50$cv.grid[,c(5,6)], col="blue")
> ##D # alternative CV estimate via reweighted LS
> ##D   points( qsreg.fit50$cv.grid[,c(2,3)], col="red")
> ## End(Not run)
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("QTps", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("RCMexample")
> ### * RCMexample
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: RCMexample
> ### Title: 3-hour precipitation fields from a regional climate model
> ### Aliases: RCMexample
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(RCMexample)
> # second time period
> 
> image.plot( RCMexample$x, RCMexample$y, RCMexample$z[,,2])
> world( add=TRUE,  lwd=2, col="grey")
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("RCMexample", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("RMprecip")
> ### * RMprecip
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: RMprecip
> ### Title: Monthly total precipitation (mm) for August 1997 in the Rocky
> ###   Mountain Region and some gridded 4km elevation data sets (m).
> ### Aliases: RMprecip RMelevation PRISMelevation
> ### Keywords: datasets
> 
> ### ** Examples
> 
> # this data set was created  the 
> # historical data  taken from 
> # Observed monthly precipitation, min and max temperatures for the coterminous US 
> # 1895-1997
> # NCAR_pinfill 
> # see the Geophysical Statistics Project datasets page for the supporting functions 
> # and details. 
> 
> # plot 
> quilt.plot(RMprecip$x, RMprecip$y)
> US( add=TRUE, col=2, lty=2)
> 
> # comparison of station elevations with PRISM gridded values
> 
> data(RMelevation)
> 
> interp.surface( RMelevation, RMprecip$x)-> test.elev
> 
> plot( RMprecip$elev, test.elev, xlab="Station elevation", 
+ ylab="Interpolation from PRISM grid")
> abline( 0,1,col="blue")
> 
> # some differences  with high elevations probably due to complex
> # topography!
> 
> #
> # view of Rockies looking from theSoutheast
> 
> save.par<- par(no.readonly=TRUE)
> 
> par( mar=c(0,0,0,0))
> 
> # fancy use of persp with shading and lighting.
> persp( RMelevation, theta=75, phi= 15, 
+           box=FALSE, axes=FALSE, xlab="", ylab="", 
+          border=NA,
+          shade=.95, lphi= 10, ltheta=80,
+          col= "wheat4", 
+          scale=FALSE, expand=.00025)
> 
> # reset graphics parameters and a more conventional image plot.
> par( save.par)
> image.plot(RMelevation, col=topo.colors(256))
> US( add=TRUE, col="grey", lwd=2)
> title("PRISM elevations (m)")
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("RMprecip", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("Tps")
> ### * Tps
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Tps
> ### Title: Thin plate spline regression
> ### Aliases: Tps fastTps
> ### Keywords: smooth
> 
> ### ** Examples
> 
> #2-d example 
> 
> fit<- Tps(ChicagoO3$x, ChicagoO3$y)  # fits a surface to ozone measurements. 
> 
> set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
> plot(fit) # four diagnostic plots of  fit and residuals. 
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> 
> # summary of fit and estiamtes of lambda the smoothing parameter
> summary(fit)
CALL:
Tps(x = ChicagoO3$x, Y = ChicagoO3$y)
                                               
 Number of Observations:                20     
 Number of unique points:               20     
 Number of parameters in the null space 3      
 Parameters for fixed spatial drift     3      
 Effective degrees of freedom:          4.5    
 Residual degrees of freedom:           15.5   
 MLE tau                                3.779  
 GCV tau                                4.073  
 MLE sigma                              347.7  
 Scale passed for covariance (sigma)    <NA>   
 Scale passed for nugget (tau^2)        <NA>   
 Smoothing parameter lambda             0.04107

Residual Summary:
    min   1st Q  median   3rd Q     max 
-6.8060 -1.4390 -0.5064  1.4440  7.7890 

Covariance Model: Rad.cov
  Names of non-default covariance arguments: 
       p

DETAILS ON SMOOTHING PARAMETER:
 Method used:   GCV    Cost:  1
   lambda       trA       GCV   GCV.one GCV.model    tauHat 
  0.04107   4.50304  21.40938  21.40938        NA   4.07296 

 Summary of all estimates found for lambda
            lambda   trA   GCV tauHat -lnLike Prof converge
GCV        0.04107 4.503 21.41  4.073        49.00        5
GCV.model       NA    NA    NA     NA           NA       NA
GCV.one    0.04107 4.503 21.41  4.073           NA        5
RMSE            NA    NA    NA     NA           NA       NA
pure error      NA    NA    NA     NA           NA       NA
REML       0.02972 4.886 21.49  4.030        48.98        4
> 
> surface( fit) # Quick image/contour plot of GCV surface.
> 
> # NOTE: the predict function is quite flexible:
> 
>      look<- predict( fit, lambda=2.0)
> #  evaluates the estimate at lambda =2.0  _not_ the GCV estimate
> #  it does so very efficiently from the Krig fit object.
> 
>      look<- predict( fit, df=7.5)
> #  evaluates the estimate at the lambda values such that 
> #  the effective degrees of freedom is 7.5
>  
> 
> # compare this to fitting a thin plate spline with 
> # lambda chosen so that there are 7.5 effective 
> # degrees of freedom in estimate
> # Note that the GCV function is still computed and minimized
> # but the lambda values used correpsonds to 7.5 df.
> 
> fit1<- Tps(ChicagoO3$x, ChicagoO3$y,df=7.5)
> 
> set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
> plot(fit1) # four diagnostic plots of  fit and residuals.
>           # GCV function (lower left) has vertical line at 7.5 df.
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> 
> # The basic matrix decompositions are the same for 
> # both fit and fit1 objects. 
> 
> # predict( fit1) is the same as predict( fit, df=7.5)
> # predict( fit1, lambda= fit$lambda) is the same as predict(fit) 
> 
> 
> # predict onto a grid that matches the ranges of the data.  
> 
> out.p<-predictSurface( fit)
> image( out.p) 
> 
> # the surface function (e.g. surface( fit))  essentially combines 
> # the two steps above
> 
> # predict at different effective 
> # number of parameters 
> out.p<-predictSurface( fit,df=10)
> 
> ## Not run: 
> ##D # predicting on a grid along with a covariate
> ##D   data( COmonthlyMet)	
> ##D # predicting average daily minimum temps for spring in Colorado
> ##D # NOTE to create an  4km  elevation grid: 
> ##D # data(PRISMelevation); CO.elev1 <- crop.image(PRISMelevation, CO.loc )
> ##D # then use same grid for the predictions: CO.Grid1<- CO.elev1[c("x","y")]
> ##D   obj<- Tps( CO.loc, CO.tmin.MAM.climate, Z= CO.elev)
> ##D   out.p<-predictSurface( obj,
> ##D             grid.list=CO.Grid, ZGrid= CO.elevGrid)
> ##D   image.plot( out.p)        
> ##D   US(add=TRUE, col="grey")
> ##D   contour( CO.elevGrid, add=TRUE, levels=c(2000), col="black")
> ## End(Not run)
> ## Not run: 
> ##D #A 1-d example  with confidence intervals
> ##D   out<-Tps( rat.diet$t, rat.diet$trt) # lambda found by GCV 
> ##D   out
> ##D   plot( out$x, out$y)
> ##D   xgrid<- seq(  min( out$x), max( out$x),,100)
> ##D   fhat<- predict( out,xgrid)
> ##D   lines( xgrid, fhat,)
> ##D   SE<- predictSE( out, xgrid)
> ##D   lines( xgrid,fhat + 1.96* SE, col="red", lty=2)
> ##D   lines(xgrid, fhat - 1.96*SE, col="red", lty=2)
> ##D 
> ##D # 
> ##D # compare to the ( much faster) B spline algorithm 
> ##D #  sreg(rat.diet$t, rat.diet$trt) 
> ##D 
> ##D # Here is a 1-d example with 95 percent  CIs  where sreg would not 
> ##D # work:
> ##D #  sreg would give the right estimate here but not the right CI's
> ##D   x<- seq( 0,1,,8)
> ##D   y<- sin(3*x)
> ##D   out<-Tps( x, y) # lambda found by GCV 
> ##D   plot( out$x, out$y)
> ##D   xgrid<- seq(  min( out$x), max( out$x),,100)
> ##D   fhat<- predict( out,xgrid)
> ##D   lines( xgrid, fhat, lwd=2)
> ##D   SE<- predictSE( out, xgrid)
> ##D   lines( xgrid,fhat + 1.96* SE, col="red", lty=2)
> ##D   lines(xgrid, fhat - 1.96*SE, col="red", lty=2)
> ## End(Not run)
> 
> # More involved example adding a covariate to the fixed part of model
> ## Not run: 
> ##D set.panel( 1,3)
> ##D # without elevation covariate
> ##D   out0<-Tps( RMprecip$x,RMprecip$y)
> ##D   surface( out0)
> ##D   US( add=TRUE, col="grey")
> ##D 
> ##D # with elevation covariate
> ##D   out<- Tps( RMprecip$x,RMprecip$y, Z=RMprecip$elev)
> ##D   
> ##D # NOTE: out$d[4] is the estimated elevation coefficient
> ##D # it is easy to get the smooth surface separate from the elevation.
> ##D   out.p<-predictSurface( out, drop.Z=TRUE)
> ##D   surface( out.p)
> ##D   US( add=TRUE, col="grey")
> ##D   
> ##D # and if the estimate is of high resolution and you get by with 
> ##D # a simple discretizing -- does not work in this case!
> ##D   quilt.plot( out$x, out$fitted.values)
> ##D   
> ##D #
> ##D # the exact way to do this is evaluate the estimate
> ##D # on a grid where you also have elevations 
> ##D # An elevation DEM from the PRISM climate data product (4km resolution)
> ##D   data(RMelevation)  
> ##D   grid.list<- list( x=RMelevation$x, y= RMelevation$y)
> ##D   fit.full<- predictSurface( out, grid.list, ZGrid= RMelevation)
> ##D   
> ##D # this is the linear fixed part of the second spatial model:
> ##D # lon,lat and elevation
> ##D   fit.fixed<- predictSurface( out, grid.list, just.fixed=TRUE,
> ##D                    ZGrid= RMelevation)
> ##D                    
> ##D # This is the smooth part but also with the linear lon lat terms. 
> ##D   fit.smooth<-predictSurface( out, grid.list, drop.Z=TRUE)
> ##D   
> ##D #
> ##D   set.panel( 3,1)
> ##D   
> ##D   fit0<- predictSurface( out0, grid.list)
> ##D   image.plot( fit0)
> ##D   title(" first spatial model (w/o elevation)")
> ##D   image.plot( fit.fixed)
> ##D   title(" fixed part of second model (lon,lat,elev linear model)")
> ##D   US( add=TRUE)
> ##D   image.plot( fit.full)
> ##D   title("full prediction second model")
> ##D   set.panel()
> ## End(Not run)
> ### 
> ### fast Tps
> # m=2   p= 2m-d= 2
> #
> # Note: aRange = 3 degrees is a very generous taper range. 
> # Use some trial aRange value with rdist.nearest to determine a
> # a useful taper. Some empirical studies suggest that in the 
> # interpolation case in 2 d the taper should be large enough to 
> # about 20 non zero nearest neighbors for every location.
> 
>   out2<- fastTps( RMprecip$x,RMprecip$y,m=2, aRange=3.0, 
+                       profileLambda=FALSE)
> 
> # note that fastTps produces a object of classes  spatialProcess and mKrig
> # so one can use all the 
> # the overloaded functions that are defined for these classes.
> # predict, predictSE, plot, sim.spatialProcess 
> # summary of what happened note estimate of effective degrees of 
> # freedom
> # profiling on lambda has been turned off to make this run quickly
> # but it is suggested that one examines the the profile likelihood over lambda
>   
>   print( out2)
CALL:
fastTps(x = RMprecip$x, Y = RMprecip$y, m = 2, aRange = 3, profileLambda = FALSE)

 SUMMARY OF MODEL FIT:
                                                             
 Number of Observations:                    806              
 Degree of polynomial in fixed part:        1                
 Total number of parameters in fixed part:  3                
 tau  Nugget stan. dev:                     25.99            
 sigma Process variance:                    22.53            
 lambda   tau^2/sigma^2:                    1.33             
 aRange parameter (in units of distance):   3                
 Approx.  degrees of freedom for curve      112.1            
    Standard Error of df estimate:          1.96             
 log Likelihood:                            -3871.12060412435
 log Likelihood REML:                       -3879.17851011222

 ESTIMATED COEFFICIENTS FOR FIXED PART:

   estimate       SE    pValue
d1  756.400 106.5000 1.202e-12
d2    4.425   0.9369 2.317e-06
d3   -5.471   1.1070 7.698e-07

 COVARIANCE MODEL: wendland.cov
   Non-default covariance arguments and their values 
k :
[1] 2
Dist.args :
$method
[1] "euclidean"

aRange :
[1] 3
Nonzero entries in covariance matrix  119816

SUMMARY FROM Max. Likelihood ESTIMATION:
Parameters found from optim: 
  lambda 
1.330438 
Approx. confidence intervals for MLE(s) 
        lower95% upper95%
lambda 0.9586872 1.846342

 Note: MLEs for  tau and sigma found analytically from lambda

Summary from estimation:
lnProfileLike.FULL lnProfileREML.FULL             lambda                tau 
      -3871.120604       -3879.178510           1.330438          25.991494 
            sigma2             aRange             eff.df                GCV 
        507.771046           3.000000         112.100986         788.576835 
> 
> ## Not run: 
> ##D set.panel( 1,2)
> ##D surface( out2)
> ##D 
> ##D #
> ##D # now use great circle distance for this smooth 
> ##D # Here "aRange" for the taper support is the great circle distance in degrees latitude.
> ##D # Typically for data analysis it more convenient to think in degrees. A degree of
> ##D # latitude is about 68 miles (111 km).
> ##D #
> ##D fastTps( RMprecip$x,RMprecip$y,m=2, lon.lat=TRUE, aRange= 210 ) -> out3
> ##D print( out3)  # note the effective degrees of freedom is different.
> ##D surface(out3)
> ##D 
> ##D set.panel()
> ## End(Not run)
> 
> ## Not run: 
> ##D #
> ##D # simulation reusing Tps/Krig object
> ##D #
> ##D fit<- Tps( rat.diet$t, rat.diet$trt)
> ##D true<- fit$fitted.values
> ##D N<-  length( fit$y)
> ##D temp<- matrix(  NA, ncol=50, nrow=N)
> ##D tau<- fit$tauHat.GCV
> ##D for (  k in 1:50){
> ##D ysim<- true + tau* rnorm(N) 
> ##D temp[,k]<- predict(fit, y= ysim)
> ##D }
> ##D matplot( fit$x, temp, type="l")
> ##D 
> ## End(Not run)
> # 
> #4-d example 
> fit<- Tps(BD[,1:4],BD$lnya,scale.type="range") 
> 
> # plots fitted surface and contours 
> # default is to hold 3rd and 4th fixed at median values 
> 
> surface(fit)   
> 
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("Tps", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("US")
> ### * US
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: US
> ### Title: Plot of the US with state boundaries
> ### Aliases: US
> ### Keywords: hplot
> 
> ### ** Examples
> 
> # Draw map in device color # 3 
> US( col=3) 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("US", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("Wendland")
> ### * Wendland
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Wendland
> ### Title: Wendland family of covariance functions and supporting numerical
> ###   functions
> ### Aliases: Wendland Wendland.beta Wendland2.2 fields.D fields.pochdown
> ###   fields.pochup wendland.eval
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> dt<- seq( 0,1.5,, 200)
> 
> y<- Wendland( dt, k=2, dimension=2)
> 
> plot( dt, y, type="l")
> 
> # should agree with 
> 
> y.test<- Wendland2.2( dt)
> points( dt, y.test)
> 
> # second derivative
> plot( dt, Wendland( dt, k=4, dimension=2, derivative=2), type="l")
> 
> # a radial basis function using the Wendland  the  "knot" is at (.25,.25)
> gl<- list( x= seq( -1,1,,60), y = seq( -1,1,,60) )
> 
> bigD<- rdist( make.surface.grid( gl), matrix( c(.25,.25), nrow=1))
> RBF<- matrix(Wendland( bigD, k=2, dimension=2), 60,60)
> 
> # perspective with some useful settings for shading.
> persp( gl$x, gl$y, RBF, theta =30, phi=20, shade=.3, border=NA, col="grey90")
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("Wendland", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("WorldBank")
> ### * WorldBank
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: WorldBankCO2
> ### Title: Carbon emissions and demographic covariables by country for
> ###   1999.
> ### Aliases: WorldBankCO2
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(WorldBankCO2)
> plot( WorldBankCO2[,"GDP.cap"], WorldBankCO2[,"CO2.cap"], log="xy")
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("WorldBank", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("add.image")
> ### * add.image
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: add.image
> ### Title: Adds an image to an existing plot.
> ### Aliases: add.image
> ### Keywords: hplot
> 
> ### ** Examples
> 
> plot( 1:10, 1:10, type="n")
> data( lennon)
> 
> add.image( 5,4,lennon, col=grey( (0:256)/256))
> # reference lines 
> xline( 5, col=2)
> yline( 4,col=2) 
> 
> #
> # add lennon right in the corner beyond the plotting region
> # 
> 
> par(new=TRUE, plt=c(0,1,0,1), mar=c(0,0,0,0), usr=c(0,1,0,1))
> add.image( 0,0, lennon, adj.x=0, adj.y=0) 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("add.image", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("arrow.plot")
> ### * arrow.plot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: arrow.plot
> ### Title: Adds arrows to a plot
> ### Aliases: arrow.plot
> ### Keywords: aplot
> 
> ### ** Examples
> 
> #
> # 20 random directions at 20 random points
> 
> x<- runif( 20)
> y<- runif( 20)
> u<- rnorm( 20)
> v<- rnorm( 20)
> plot( x,y)
> arrow.plot( x,y,u,v) # a default that is unattractive 
> 
> plot( x,y, type="n")
> arrow.plot( x,y,u,v, arrow.ex=.2, length=.1, col='green', lwd=2) 
> # thicker lines in green, smaller heads and longer tails. Note length, col and lwd are
> # options that the arrows function itself knows about. 
>   
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("arrow.plot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("as.image")
> ### * as.image
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: as.image
> ### Title: Creates image from irregular x,y,z
> ### Aliases: as.image
> ### Keywords: manip
> 
> ### ** Examples
> 
> # convert precip data to 50X50 image  
> look<- as.image( RMprecip$y, x= RMprecip$x, nx=50, ny=50)
> image.plot( look) 
> 
> # reduced grid extent compared to the domain
> gridList<- list( x = seq(-105,-101,length.out=10),
+                  y = seq(  38, 42,length.out=10) )
> look2<- as.image( RMprecip$y, x= RMprecip$x,grid=gridList)
Warning in discretize.image(x, m = nx, n = ny, grid = grid, boundary.grid = boundary.grid) :
  Some locations are outside the range of the grid
> image.plot( look2) 
> 
> # number of obs in each cell -- in this case equal to the 
> # aggregated weights because each obs had equal weight in the call
> 
> image.plot( look$x ,look$y, look$weights, col=terrain.colors(50)) 
> # hot spot is around Denver
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("as.image", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("as.surface")
> ### * as.surface
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: as.surface
> ### Title: Creates an "surface" object from grid values.
> ### Aliases: as.surface
> ### Keywords: manip
> 
> ### ** Examples
> 
>  
> 
> # Make a perspective of the surface Z= X**2 -Y**2 
> # Do this by evaluating quadratic function on a 25 X 25 grid
>   
> grid.l<-list( abcissa= seq( -2,2,,15), ordinate= seq( -2,2,,20)) 
> xg<-make.surface.grid( grid.l)
> # xg is a 300X2 matrix that has all pairs of X and Y grid values 
> z<- xg[,1]**2 - xg[,2]**2  
> # now fold z in the matrix format needed for persp 
> out.p<-as.surface( xg, z) 
> persp( out.p) 
> # also try  plot( out.p) to see the default plot for a surface object 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("as.surface", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("bplot")
> ### * bplot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: bplot
> ### Title: boxplot
> ### Aliases: bplot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> #
> set.seed(123)
> temp<- matrix( rnorm(12*8), ncol=12)
> pos<- c(1:6,9, 12:16)*100
> bplot(temp)
> #
> par(las=2)
> bplot( temp, pos=pos, names=paste( "Data",1:12, sep=""))
> # add an axis along top for reference
> axis(3)
> 
> #
> # Xmas boxplots in pleasing red and green 
> bplot( temp, pos=pos,  col=c("red4", "green4"))
> # add an axis on top
> axis( 3)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("bplot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("bplot.xy")
> ### * bplot.xy
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: bplot.xy
> ### Title: Boxplots for conditional distribution
> ### Aliases: bplot.xy
> ### Keywords: hplot
> 
> ### ** Examples
> 
> # condition on swim times to see how run times vary
> bplot.xy( minitri$swim, minitri$run, N=5)
> 
> # bivariate normal corr= .8
> set.seed( 123)
> x<-rnorm( 2000)
> y<- .8*x +  sqrt( 1- .8**2)*rnorm( 200)
> #
> bplot.xy(x,y)
> #
> bplot.xy( x,y, breaks=seq( -3, 3,,25) ,
+                 xlim =c(-4,4), ylim =c(-4,4), col="grey80", lwd=2)
> points( x,y,col=3, cex=.5)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("bplot.xy", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("colorbar.plot")
> ### * colorbar.plot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: colorbar.plot
> ### Title: Adds color scale strips to an existing plot.
> ### Aliases: colorbar.plot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> # set up a plot but don't plot points  and no "box"
> plot( 1:10, (1:10)*10, type="n", bty="n") 
> # of course this could be anything 
> 
> y<- cbind( 1:15, (1:15)+25)
> 
> colorbar.plot( 2.5, 30, y)
> points( 2.5,30, pch="+", cex=2, adj=.5)
> # note that strip is still in 1:8 aspect even though plot has very 
> # different ranges for x and y. 
> 
> # adding legend using image.plot
> zr<- range( c( y))
> image.plot( legend.only=TRUE, zlim= zr) 
> # see help(image.plot) to create more room in margin etc. 
> 
> zr<- rbind( c(1,20), c(1,100)) # separate ranges for columns of y. 
> colorbar.plot( 5, 70, y, adj.x=0, zrange= zr)
> # some reference lines to show placement
> xline( 5, lty=2) # strip starts at x=5 
> yline(70, lty=2)  # strip is centered around y=7 (because adj.y=.5 by default)
> 
> # many strips on common scale.
> 
> y<- matrix( 1:200, ncol=10)
> colorbar.plot( 2, 75, y, horizontal=FALSE, col=rainbow(256)) 
> 
> # Xmas strip
> y<- cbind( rep( c(1,2),10))
> y[15] <- NA # NA's should work 
> colorbar.plot( 6, 45, y, adj.y=1,col=c("red", "green"))
> text(6,48,"Christmas strip", cex=2)
> 
> # lennon thumbnail
> # there are better ways to this ... see add.image for example.
> data( lennon)
> colorbar.plot( 7.5,22, lennon, 
+            strip.width=.25, strip.length=.25, col=grey(seq( 0,1,,256)))
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("colorbar.plot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("compactToMat")
> ### * compactToMat
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: compactToMat
> ### Title: Convert Matrix from Compact Vector to Standard Form
> ### Aliases: compactToMat
> ### Keywords: compact matrix
> 
> ### ** Examples
> 
> ################
> #Calculate distance matrix from compact form:
> ################
> 
> #make a distance matrix
> distOut = rdist(1:5, compact=TRUE)
> print(distOut)
  1 2 3 4
2 1      
3 2 1    
4 3 2 1  
5 4 3 2 1
> 
> #note that distOut is in compact form:
> print(c(distOut))
 [1] 1 2 3 4 1 2 3 1 2 1
> 
> #convert to standard matrix form:
> distMat = compactToMat(distOut)
> 
> ################
> #fast computation of covariance matrix:
> ################
> 
> #generate 5 random points on [0,1]x[0,1] square
> x = matrix(runif(10), nrow=5)
> 
> #get compact distance matrix
> distOut = rdist(x, compact=TRUE)
> 
> #evaluate Exponential covariance with range=1.  Note that
> #Covariance function is only evaluated over upper triangle
> #so time is saved.
> diagVal = Exponential(0, range=1)
> compactCovMat = Exponential(distOut, range=1)
> upperCovMat = compactToMat(compactCovMat, diagVal)
> lowerCovMat = compactToMat(compactCovMat, diagVal, lower.tri=TRUE, upper.tri=FALSE)
> fullCovMat = compactToMat(compactCovMat, diagVal, lower.tri=TRUE, upper.tri=TRUE)
> compactCovMat
          1         2         3         4
2 0.8902714                              
3 0.6780921 0.7063275                    
4 0.4981622 0.5368347 0.7140175          
5 0.4321275 0.4068989 0.4942635 0.4040921
> lowerCovMat
          [,1]      [,2]      [,3]      [,4] [,5]
[1,] 1.0000000 0.0000000 0.0000000 0.0000000    0
[2,] 0.8902714 1.0000000 0.0000000 0.0000000    0
[3,] 0.6780921 0.7063275 1.0000000 0.0000000    0
[4,] 0.4981622 0.5368347 0.7140175 1.0000000    0
[5,] 0.4321275 0.4068989 0.4942635 0.4040921    1
> upperCovMat
     [,1]      [,2]      [,3]      [,4]      [,5]
[1,]    1 0.8902714 0.6780921 0.4981622 0.4321275
[2,]    0 1.0000000 0.7063275 0.5368347 0.4068989
[3,]    0 0.0000000 1.0000000 0.7140175 0.4942635
[4,]    0 0.0000000 0.0000000 1.0000000 0.4040921
[5,]    0 0.0000000 0.0000000 0.0000000 1.0000000
> fullCovMat
          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 1.0000000 0.8902714 0.6780921 0.4981622 0.4321275
[2,] 0.8902714 1.0000000 0.7063275 0.5368347 0.4068989
[3,] 0.6780921 0.7063275 1.0000000 0.7140175 0.4942635
[4,] 0.4981622 0.5368347 0.7140175 1.0000000 0.4040921
[5,] 0.4321275 0.4068989 0.4942635 0.4040921 1.0000000
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("compactToMat", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("cover.design")
> ### * cover.design
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: cover.design
> ### Title: Computes Space-Filling "Coverage" designs using Swapping
> ###   Algorithm
> ### Aliases: cover.design
> ### Keywords: spatial
> 
> ### ** Examples
> 
> ##
> ## 
> # first generate candidate set
> set.seed(123) # setting seed so that you get the same thing I do!
> test.df <- matrix( runif( 600), ncol=3)
> 
> test1.des<-cover.design(R=test.df,nd=10)
> 
> summary( test1.des)
Call:
cover.design(R = test.df, nd = 10)
                               
 Number of design points: 10   
 Number of fixed points:  0    
 Optimality Criterion:    0.406

History:
 step swap.out swap.in new.crit
    0        0       0   0.7055
    1      188      75   0.6510
    2      190     161   0.5859
    3      137     194   0.5356
    4      139      67   0.5137
    5       43     146   0.5116
    6      115      42   0.5109
    7      158     123   0.5007
    8      189      84   0.4975
    9       55      38   0.4731
   10      193     138   0.4675
   11       75     120   0.4510
   12      161      29   0.4288
   13      194      97   0.4217
   14      146      45   0.4151
   15       42      36   0.4145
   16      123     187   0.4078
   17       38      46   0.4060
> plot( test1.des)
> 
> #
> candidates<- make.surface.grid( list( seq( 0,5,,20), seq(0,5,,20)))
> out<- cover.design( candidates, 15)
> 
> # find 10 more points keeping this original design fixed
> 
> out3<-cover.design( candidates, 10,fixed=out$best.id)
> # see what happened
> 
> plot( candidates[,1:2], pch=".")
> points( out$design, pch="x")
> points( out3$design, pch="o")    
> 
> # here is a strange graph illustrating the swapping history for the
> # the first design. Arrows show the swap done  
> # at each pass through the design.
> 
> h<- out$history
> cd<- candidates
> plot( cd[,1:2], pch=".")
> points( out$design, pch="O", col=2)
> points( out$start.design, pch="x", col=5)  
> 
> arrows(
+ cd[h[,2],1],
+ cd[h[,2],2],
+ cd[h[,3],1],
+ cd[h[,3],2],length=.1)
> text( cd[h[,2],1],
+ cd[h[,2],2], h[,1], cex=1.0 )
>                                
> 
> #
> # try this out using "Manhattan distance"
> #  ( distance following a grid of city streets)
> 
> dist.man<- function(x1,x2) {
+             d<- ncol( x1)
+             temp<- abs(outer( x1[,1], x2[,1],'-'))
+             for ( k in 2:d){
+                temp<- temp+abs(outer( x1[,k], x2[,k],'-'))
+             }
+             temp }
> 
> # use the design from the Euclidean distance as the starting
> #configuration.
> 
> cover.design( candidates, 15, DIST=dist.man, start= out3$best.id)-> out2
> # this takes a while ...
> plot( out2$design)
> points( out3$design, col=2)
> 
> # find a design on the sphere
> #
> 
> candidates<- make.surface.grid( list( x=seq( -180,180,,20), y= seq( -85,
+ 85,,20)))
> 
> out4<-cover.design( candidates, 15, DIST=rdist.earth)
> # this takes a while 
> 
> plot( candidates, pch="+", cex=2)
> points(out4$design, pch="o", cex=2, col="blue")
> 
> # covering based on correlation for 153 ozone stations
> #
> data( ozone2)
> 
> cor.mat<-cor( ozone2$y, use="pairwise")
> 
> cor.dist<- function( x1,x2)
+ {matrix( 1-cor.mat[ x1,x2], ncol=length(x2))}
> 
> #
> # find 25 points out of the 153
> # here the "locations" are just the index but the distance is 
> # determined by the correlation function. 
> #
> out5<-cover.design(cbind(1:153),25, DIST= cor.dist, scale.type="unscaled") 
> 
> plot( ozone2$lon.lat, pch=".")
> points(  ozone2$lon.lat[out5$best.id,],pch="O", col=4)
> #
> # this seems a bit strange probably due some funny correlation values
> #
> 
> # reset panel
> set.panel(1,1)
plot window will lay out plots in a 1 by 1 matrix 
>  
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("cover.design", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("drape.plot")
> ### * drape.plot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: drape.plot
> ### Title: Perspective plot draped with colors in the facets.
> ### Aliases: drape.color drape.plot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
> # an obvious choice:
> # Dr. R's favorite New Zealand Volcano!
> data( volcano)
> M<- nrow( volcano)
> N<- ncol( volcano)
> x<- seq( 0,1,,M)
> y<- seq( 0,1,,N)
> 
> pm<- drape.plot( x,y,volcano, col=terrain.colors(128)) 
> 
> # use different range for color scale and persp plot
> # setting of border omits the mesh lines
> 
>  drape.plot( x,y,volcano, col=topo.colors(128),zlim=c(0,300),
+                      zlim2=c( 120,200), border=NA)
> 
> # note tranparent color for facets outside the zlim2 range
> 
> #The projection has been saved in pm
> # add a point marking the summit
> zsummit <- max( volcano) 
> ix<- row( volcano)[volcano==zsummit]
> iy <- col( volcano)[volcano==zsummit]
> uv <- trans3d( x[ix], y[iy],zsummit,pm)
> points( uv, col="magenta", pch="+", cex=2)
> 
> # overlay volcano wireframe with gradient in x direction. 
> 
> dz<- ( 
+      volcano[1:(M-1), 1:(N-1)] - volcano[2:(M), 1:(N-1)] +
+      volcano[1:(M-1), 2:(N)] - volcano[2:(M), 2:(N)]  
+          )/2
> 
> # convert dz to a color scale:
>   zlim<- range( c( dz), na.rm=TRUE)
>   zcol<-drape.color( dz, zlim =zlim, col = viridis(64) )$color.index
> 
> # with these colors 
> 
>   persp( volcano, col=zcol, theta=30, phi=20,
+   border=NA,expand=.3 )
> 
> # add legend using image.plot function 
>   image.plot( zlim=zlim, legend.only =TRUE, horizontal =TRUE,
+             col= viridis(64))
>  
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("drape.plot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("envelopePlot")
> ### * envelopePlot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: envelopePlot
> ### Title: Add a shaded the region between two functions to an existing
> ###   plot
> ### Aliases: envelopePlot
> 
> ### ** Examples
> 
> x <- seq(0, 2*pi,, 100)
> y1 <- cos(x)
> y2 <- sin(x)
> plot(x, y1, type="l")
> envelopePlot(x, y1, y2=y2)
> 
> x1 <- c(0, 0.5, 1)
> y1 <- c(0, 2, 1)
> x2 <- c(0, 1)
> y2 <- c(-1, 0)
> plot(x1, y1, type="l", ylim=c(-1, 2))
> envelopePlot(x1, y1, x2, y2)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("envelopePlot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("exp.cov")
> ### * exp.cov
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Covariance functions
> ### Title: Exponential family, radial basis functions,cubic spline,
> ###   compactly supported Wendland family and stationary covariances.
> ### Aliases: Exp.cov Exp.simple.cov Rad.cov Rad.simple.cov stationary.cov
> ###   stationary.taper.cov wendland.cov cubic.cov
> ### Keywords: spatial
> 
> ### ** Examples
> 
> # exponential covariance matrix ( marginal variance =1) for the ozone
> #locations 
> out<- Exp.cov( ChicagoO3$x, aRange=100)
> 
> # out is a 20X20 matrix
> 
> out2<- Exp.cov( ChicagoO3$x[6:20,],ChicagoO3$x[1:2,], aRange=100)
> # out2 is 15X2 matrix 
> 
> # Kriging fit where the nugget variance is found by GCV 
> # Matern covariance shape with range of 100.
> # 
> 
> fit<- Krig( ChicagoO3$x, ChicagoO3$y, Covariance="Matern", aRange=100,smoothness=2)
> 
> data( ozone2)
> x<- ozone2$lon.lat
> y<- ozone2$y[16,]
> # Omit the NAs
> good<- !is.na( y)
> x<- x[good,]
> y<- y[good]
> 
> 
> # example of calling the taper version directly 
> # Note that default covariance is exponential and default taper is 
> # Wendland (k=2).
> 
> stationary.taper.cov( x[1:3,],x[1:10,] , aRange=1.5, Taper.args= list(k=2,aRange=2.0,
+                        dimension=2) )-> temp
> # temp is now a tapered 3X10 cross covariance matrix in sparse format. 
> 
>  is.spam( temp)  # evaluates to TRUE
[1] TRUE
> 
> # should be identical to
> # the direct matrix product
> 
>  temp2<- Exp.cov( x[1:3,],x[1:10,], aRange=1.5) * Wendland(rdist(x[1:3,],x[1:10,]), 
+                       aRange= 2.0, k=2, dimension=2)
>  test.for.zero(  as.matrix(temp), temp2)
PASSED test at tolerance  1e-08
> 
> # Testing that the "V" option works as advertized ...
> x1<- x[1:20,]
> x2<- x[1:10,]
> 
> V<- matrix( c(2,1,0,4), 2,2)
> Vi<- solve( V)
> 
> u1<- t(Vi%*% t(x1))
> u2<- t(Vi%*% t(x2))
> 
> look<- exp(-1*rdist(u1,u2))
> look2<- stationary.cov( x1,x2, V= V)
> test.for.zero( look, look2)
PASSED test at tolerance  1e-08
> 
> 
> # Here is an example of how the cross covariance multiply works
> # and lots of options on the arguments
> 
> 
>  Ctest<- rnorm(10)
>  
>  temp<- stationary.cov( x,x[1:10,], C= Ctest, 
+         Covariance= "Wendland", 
+             k=2, dimension=2, aRange=1.5 )
> 
> # do multiply explicitly
> 
>  temp2<- stationary.cov( x,x[1:10,],
+         Covariance= "Wendland",
+             k=2, dimension=2, aRange=1.5 )%*% Ctest
> 
>  test.for.zero( temp, temp2)
PASSED test at tolerance  1e-08
> 
> 
> # use the tapered stationary version 
> # cov.args is part of the argument list passed to stationary.taper.cov
> # within Krig. 
> # This example needs the spam package.
> # 
> 
> ## Not run: 
> ##D 
> ##D Krig(x,y, cov.function = "stationary.taper.cov", aRange=1.5,
> ##D       cov.args= list(Taper.args= list(k=2, dimension=2,aRange=2.0) )
> ##D            ) -> out2 
> ##D # NOTE: Wendland is the default taper here. 
> ## End(Not run)
> 
> # BTW  this is very similar to 
> ## Not run: 
> ##D  Krig(x,y, aRange= 1.5)-> out
> ## End(Not run)
> 
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("exp.cov", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("fields")
> ### * fields
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: fields
> ### Title: fields - tools for spatial data
> ### Aliases: fields-package fields
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # some air quality data, daily surface ozone measurements for the Midwest:
> ##D data(ozone2)
> ##D x<-ozone2$lon.lat
> ##D y<- ozone2$y[16,] # June 18, 1987
> ##D # (artifically) reduce data size for CRAN ...
> ##D 
> ##D 
> ##D # pixel plot of spatial data
> ##D quilt.plot( x,y)
> ##D US( add=TRUE) # add US map
> ##D 
> ##D fit<- Tps(x,y)
> ##D # fits a GCV thin plate smoothing spline surface to ozone measurements.
> ##D # Hey, it does not get any easier than this!
> ##D 
> ##D summary(fit) #diagnostic summary of the fit 
> ##D set.panel(2,2)
> ##D plot(fit) # four diagnostic plots of fit and residuals.
> ##D 
> ##D # quick plot of predicted surface
> ##D set.panel()
> ##D surface(fit) # contour/image plot of the fitted surface
> ##D US( add=TRUE, col="magenta", lwd=2) # US map overlaid
> ##D title("Daily max 8 hour ozone in PPB,  June 18th, 1987")
> ##D 
> ##D 
> ##D fit2<- spatialProcess( x,y)
> ##D # a "Kriging" model. The covariance defaults to a Matern with smoothness 1.0.
> ##D # the nugget, sill and range parameters are found by maximum likelihood
> ##D # summary, plot, and surface also work for  fit2 !
> ##D 
> ## End(Not run)
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("fields", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("fields.grid")
> ### * fields.grid
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: fields.grid
> ### Title: Using MKrig for predicting on a grid.
> ### Aliases: mKrig.grid
> ### Keywords: hplot
> 
> ### ** Examples
> 
> x<- RMprecip$x
> y<- RMprecip$y
> 
> Tps( x,y)-> obj
> 
> # make up an 80X80 grid that has ranges of observations
> # use same coordinate names as the x matrix
> 
> glist<- fields.x.to.grid(x, nx=80, ny=80) # this is a cute way to get a default grid that covers x
> 
> # convert grid list to actual x and y values ( try plot( Bigx, pch="."))
>     make.surface.grid(glist)-> Bigx 
> 
> # include actual x locations along with grid. 
>     Bigx<- rbind( x, Bigx)
> 
> # evaluate the surface on this set of points (exactly)
> 
>     predict(obj, x= Bigx)-> Bigy
> 
> # set the range for the compact covariance function 
> # this will involve  less than 20 nearest neighbors that have
> # nonzero covariance
> # 
> 
>      V<- diag(c( 2.5*(glist$lon[2]-glist$lon[1]), 
+                  2.5*(glist$lat[2]-glist$lat[1])))
> ## Not run: 
> ##D # this is an interplotation of the values using a compact 
> ##D # but thin plate spline like covariance. 
> ##D     mKrig( Bigx,Bigy, cov.function="wendland.cov",k=4, V=V, 
> ##D                  lambda=0)->out2 
> ##D # the big evaluation this takes about 45 seconds on a Mac G4 latop
> ##D     predictSurface( out2, nx=400, ny=400)-> look
> ## End(Not run)
> 
> # the nice surface
> ## Not run: 
> ##D     
> ##D     surface( look)
> ##D     US( add=TRUE, col="white")
> ## End(Not run)
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("fields.grid", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("grid.list")
> ### * grid.list
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: grid list
> ### Title: Some simple functions for working with gridded data and the grid
> ###   format (grid.list) used in fields.
> ### Aliases: 'grid list' grid.list fields.x.to.grid parse.grid.list
> ###   fields.convert.grid discretize.image make.surface.grid unrollZGrid
> ###   makeMultiIndex
> ### Keywords: misc
> 
> ### ** Examples
> 
> 
> #Given below are some examples of grid.list objects and the results  
> #when they are used with make.surface.grid. Note that  
> #make.surface.grid returns a matrix that retains the grid.list  
> #information as an attribute. 
> 
> grid.l<- list( 1:3, 2:5) 
> make.surface.grid(grid.l)
      [,1] [,2]
 [1,]    1    2
 [2,]    2    2
 [3,]    3    2
 [4,]    1    3
 [5,]    2    3
 [6,]    3    3
 [7,]    1    4
 [8,]    2    4
 [9,]    3    4
[10,]    1    5
[11,]    2    5
[12,]    3    5
attr(,"grid.list")
attr(,"grid.list")[[1]]
[1] 1 2 3

attr(,"grid.list")[[2]]
[1] 2 3 4 5

> 
>   
> grid.l <- list( 1:3, 10, 1:3) 
> make.surface.grid(grid.l) 
      [,1] [,2] [,3]
 [1,]    1   10    1
 [2,]    2   10    1
 [3,]    3   10    1
 [4,]    1   10    2
 [5,]    2   10    2
 [6,]    3   10    2
 [7,]    1   10    3
 [8,]    2   10    3
 [9,]    3   10    3
attr(,"grid.list")
attr(,"grid.list")[[1]]
[1] 1 2 3

attr(,"grid.list")[[2]]
[1] 10

attr(,"grid.list")[[3]]
[1] 1 2 3

> 
> #The next  example shows how the grid.list can be used to  
> #control surface plotting and evaluation of an estimated function. 
> # first create a test function  
> 
> set.seed( 124)
> 
> X<- 2*cbind( runif(30), runif(30), runif(30)) -1
>   
> dimnames( X)<- list(NULL, c("X1","X2","X3")) 
> y<- X[,1]**2 + X[,2]**2 + exp(X[,3])   
> 
> # fit an  interpolating thin plate spline  
> out<- Tps( X,y) 
Warning: 
Grid searches over lambda (nugget and sill variances) with  minima at the endpoints: 
  (GCV) Generalized Cross-Validation 
   minimum at  right endpoint  lambda  =  0.0005215037 (eff. df= 28.49999 )
> 
> grid.l<- list( X1= seq( 0,1,,20), X2=.5, X3=seq(0,1,,25)) 
> surface( out, grid.list=grid.l) 
> #  surface plot based on a 20X25 grid in X1 an X3  
> #                       over the square [0,2] and [0,2]   
> #                       holding X2 equal to 1.0. 
> #
> 
> 
> # test of discretize to make sure points on boundaries are counted right
> set.seed(123)
> x<- matrix( runif(200), 100,2)
> look<- discretize.image( x, m=2,n=2)
> xc<- seq(min(x[,1]), max(x[,1]),,5)
> xc<- xc[2:4]
> yc<- seq(min(x[,2]), max(x[,2]),,5)
> yc<- yc[2:4]
> grid <-  list( x= xc, y= yc)
> look2<- discretize.image( x, m=2,n=2)
> 
> table( look$index )
      index2
index1  1  2
     1 25 28
     2 25 22
> table( look2$index )
      index2
index1  1  2
     1 25 28
     2 25 22
> 
> # indicator image of discretized locations
> look<- discretize.image( RMprecip$x, m=15, n=15)
> image.plot( look$grid$x, look$grid$y,look$hist )  
> # actual locations
> points( RMprecip$x,col="magenta", pch=".") 
> 
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("grid.list", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("image.cov")
> ### * image.cov
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: image.cov
> ### Title: Exponential, Matern and general covariance functions for 2-d
> ###   gridded locations.
> ### Aliases: stationary.image.cov Exp.image.cov Rad.image.cov
> ###   wendland.image.cov matern.image.cov
> ### Keywords: spatial
> 
> ### ** Examples
> 
> # multiply 2-d isotropic exponential with aRange=4 by a random vector 
> 
> junk<- matrix(rnorm(100*100), 100,100)
> 
> cov.obj<- stationary.image.cov( setup=TRUE, 
+              grid=list(x=1:100,y=1:100),aRange=8) 
> result<-  stationary.image.cov(Y=junk,cov.obj=cov.obj)
> 
> image( matrix( result, 100,100)) # NOTE that is also a smoother!
> 
> # to do it again, no setup is needed 
> #  e.g. 
> #  junk2<- matrix(rnorm(100**2, 100,100))
> #  result2<-  stationary.image.cov(Y=junk2, cov.obj=cov.obj)
> 
> # generate a grid and set of indices based on discretizing the locations
> # in the precip dataset
> 
>  out<-as.image( RMprecip$y, x= RMprecip$x)
>  ind1<- out$ind
>  grid<- list( x= out$x, y=out$y)
> 
> #
> # discretized x locations  to use for comparison
>   xd<- cbind( out$x[ out$ind[,1]], out$y[ out$ind[,2]] )
> 
> # setup to create cov.obj for exponential covariance with range= 1.25
> 
>  cov.obj<- stationary.image.cov( setup=TRUE, grid=grid, aRange=1.25) 
> 
> # multiply covariance matrix by an arbitrary vector
>  junk<-  rnorm(nrow( ind1))
>  result<- stationary.image.cov( ind1, ind1, Y= junk,cov.obj=cov.obj)
> 
> # The brute force way would be  
> #   result<- stationary.cov( xd, xd, aRange=1.25, C=junk)
> # or 
> #   result<- stationary.cov( xd, xd, aRange=1.25) %*% junk
> # both of these take much longer 
> 
> 
> # evaluate the covariance between all grid points and the center grid point
>  Y<- matrix(0,cov.obj$m, cov.obj$n)
>  Y[32,32]<- 1
>  result<- stationary.image.cov( Y= Y,cov.obj=cov.obj)
> # covariance surface with respect to the grid point at (32,32)
> # 
> # reshape "vector" as an image
>  temp<-  matrix( result, cov.obj$m,cov.obj$n)
>  image.plot(cov.obj$grid$x,cov.obj$grid$y, temp)
> # or persp( cov.obj$grid$x,cov.obj$grid$y, temp) 
> 
> # check out the Matern
>  grid<- list( x= seq(-105,-99,,64), y= seq( 40,45,,64)) 
>  cov.obj<- matern.image.cov( 
+              setup=TRUE, grid=grid, aRange=.55, smoothness=1.0)
>  Y<- matrix(0,64,64)
>  Y[16,16]<- 1
> 
>  result<- matern.image.cov( Y= Y,cov.obj=cov.obj)
>   temp<-  matrix( result, cov.obj$m,cov.obj$n)
>  image.plot( cov.obj$grid$x,cov.obj$grid$y, temp)
> 
> # Note we have centered at the location (grid$x[16],grid$y[16]) for this case
> #  using sim.rf to simulate an Matern field
>   look<- sim.rf( cov.obj)
>   image.plot( grid$x, grid$y, look)
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("image.cov", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("image.plot")
> ### * image.plot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: image.plot
> ### Title: Draws an image plot with a legend strip for the color scale
> ###   based on either a regular grid or a grid of quadrilaterals.
> ### Aliases: image.plot setupLegend addLegend
> ### Keywords: hplot
> 
> ### ** Examples
> 
>   x<- 1:10
>   y<- 1:15
>   z<- outer( x,y,"+") 
>   image.plot(x,y,z) 
> 
> # or 
>   obj<- list( x=x,y=y,z=z)
>   image.plot(obj, legend.lab="Sverdrups")
>   
> ################################################################ 
> # the next sequence of examples explain how to quickly 
> # adpat this basic plot to include morre features
> # In another direction see the very last example where 
> # we use many of the setting in base R graphic to mimic a 
> # (beautiful) ggplot version. 
> ###############################################################
> #
> # add some points on diagonal using standard plot function
> #(with some clipping beyond 10 anticipated)
> 
>   points( 5:12, 5:12, pch="X", cex=3)
>   
> # in general image.plot will reset the plot window so you
> # can add any feature that normally works in base R
> # e.g. lines, text, contour, boxplots, ....
> #
> # adding breaks and distinct colors for intervals of z
> # with and without lab.breaks
> 
>   brk<- quantile( c(z))
>   image.plot(x,y,z, breaks=brk, col=rainbow(4))
>   
> # annotate legend strip with the  break point values and add a label
> 
>   image.plot(x,y,z, breaks=brk, col=rainbow(4),
+                        lab.breaks=names(brk))
> #
> # compare to 
> 
>   zp <-quantile(c(z), c( .05, .1,.5, .9,.95))
>   image.plot(x,y,z, 
+      axis.args=list( at=zp, labels=names(zp) ) )
>      
> # a log scaling for the colors
> 
>   ticks<- c( 1, 2,4,8,16,32)
>   image.plot(x,y,log(z), axis.args=list( at=log(ticks), labels=ticks))
> 
> # see help file for designer.colors to generate a color scale that adapts to 
> # quantiles of z. 
> # Add some color scales together here is an example of  5 blues to white to 5 reds
> # with white being a specific size.
>  colorTable<- designer.colors(11, c( "blue","white", "red") )
> # breaks with a gap of 10 to 17 assigned the white color
>  brks<- c(seq( 1, 10,,6), seq( 17, 25,,6)) 
>  image.plot( x,y,z,breaks=brks, col=colorTable)
> #
> #fat (5 characters wide) and short (50% of figure)  color bar on the bottom
>    image.plot( x,y,z,legend.width=5, legend.shrink=.5, horizontal=TRUE) 
> 
> # adding a label with all kinds of additional arguments.
> # use side=4 for vertical legend and side= 1 for horizontal legend
> # to be parallel to axes. See help(mtext).
> 
> image.plot(x,y,z, 
+        legend.args=list( text="unknown units",
+      col="magenta", cex=1.5, side=4, line=2))
>      
> # and finally add some grid lines
>  dx <- x[2] - x[1]  
>  dy <- y[2] - y[1]  
>  xtemp<- seq(  min( x)- dx/2, max(x)+ dx/2,
+          length.out = length(x) +1) 
>  ytemp<- seq(  min( y)- dy/2, max(y)+ dy/2,
+          length.out = length(y) +1)
>  xline( xtemp, col="grey", lwd=2)
>  yline( ytemp, col="grey", lwd=2)
> 
> ###############################################################
> #### example using an irregular quadrilateral grid
> ###############################################################
> data( RCMexample)
> 
> image.plot( RCMexample$x, RCMexample$y, RCMexample$z[,,1])
> ind<- 50:75 # make a smaller image to show bordering lines
> image.plot( RCMexample$x[ind,ind], RCMexample$y[ind,ind], RCMexample$z[ind,ind,1],
+                                       border="grey50", lwd=2)
> 
> ###############################################################
> #### multiple images with a common legend
> ###############################################################
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> 
> # Here is quick but quirky way to add a common legend to several plots.
> # The idea is leave some room in the margin and then at the end 
> # overplot the legend in this margin
> 
> par(oma=c( 0,0,0,4)) # margin of 4 spaces width at right hand side
> set.panel( 2,2) # 2X2 matrix of plots
plot window will lay out plots in a 2 by 2 matrix 
> 
> # now draw all your plots using usual image command
> for (  k in 1:4){
+   data<- matrix( rnorm(150), 10,15)
+   image( data, zlim=c(-4,4), col=tim.colors())
+ # and just for fun add a contour plot  
+   contour( data, add=TRUE)
+ }
> 
> par(oma=c( 0,0,0,1))# reset margin to be much smaller.
> image.plot( legend.only=TRUE, zlim=c(-4,4)) 
> 
> # image.plot tricked into  plotting in margin of old setting 
> 
> set.panel() # reset plotting device
plot window will lay out plots in a 1 by 1 matrix 
> 
> #
> # Here is a more learned strategy to add a common legend to a panel of
> # plots  consult the split.screen help file for more explanations.
> # For this example we draw two
> # images top and bottom and add a single legend color bar on the right side 
> 
> # first divide screen into the figure region (left) and legend region (right)
>    split.screen( rbind(c(0, .8,0,1), c(.8,1,0,1)))
[1] 1 2
> 
> # now subdivide up the figure region into two parts
>    split.screen(c(2,1), screen=1)-> ind
>    zr<- range( 2,35)
> # first image
>    screen( ind[1])
>    image( x,y,z, col=tim.colors(), zlim=zr)
> 
> # second image
>    screen( ind[2])
>    image( x,y,z+10, col=tim.colors(), zlim =zr)
> 
> # move to skinny region on right and draw the legend strip 
>    screen( 2)
>    image.plot( zlim=zr,legend.only=TRUE, smallplot=c(.1,.2, .3,.7),
+    col=tim.colors())
> 
>    close.screen( all=TRUE)
> 
> 
> # you can always add a legend arbitrarily to any plot;
> # note that here the plot is too big for the vertical strip but the
> # horizontal fits nicely.
> plot( 1:10, 1:10)
> image.plot( zlim=c(0,25), legend.only=TRUE)
> image.plot( zlim=c(0,25), legend.only=TRUE, horizontal =TRUE)
> 
> # combining the  usual image function and adding a legend
> # first change margin for some more room
> ## Not run: 
> ##D par( mar=c(10,5,5,5))
> ##D image( x,y,z, col=topo.colors(64))
> ##D image.plot( zlim=c(0,25), nlevel=64,legend.only=TRUE, horizontal=TRUE,
> ##D col=topo.colors(64))
> ## End(Not run)
> #
> 
> # adding a legend by  automatically making room. 
>   setupLegend()
>   colTab<- rainbow(10)
>   plot( 1:10, 1:10, col=colTab, pch=16)
>   addLegend(col=colTab, zlim = c(1,10))
> #
> 
> #######################################################
> ##### Comparison to ggplot
> #######################################################
> # the following example was created as way avoid doing more important
> # things
> ## Not run: 
> ##D library( viridis)
> ##D library(ggplot2)
> ##D 
> ##D x<- 1:20
> ##D y<-  1:24
> ##D z<- outer( x, y, "+")
> ##D 
> ##D 
> ##D # ggplot version 
> ##D   mesh<- expand.grid( x= x, y=y)
> ##D   mesh$z <- c(z)
> ##D   ggplot( data=mesh, aes( x=x, y=y, fill=z)) + 
> ##D     geom_raster(interpolate= FALSE)  + 
> ##D     scale_fill_continuous(type = "viridis")  +
> ##D     theme_bw()
> ##D 
> ##D 
> ##D # inflate range to give a margin around image
> ##D   xr<- range(x) +  c(-.08, .08)* diff( range(x))
> ##D   yr<- range(y) +  c(-.08, .08)* diff( range(y))
> ##D   
> ##D # changing these graphics parameters tends to push 
> ##D # text closer to the axes. 
> ##D   par( mgp=c(1.5,.5,0),mar=c(2.5,2.5,.5,1), cex=.8)
> ##D   
> ##D   image.plot(x,y,z, 
> ##D              col = viridis(128), 
> ##D    legend.shrink = .27,
> ##D             xlim = xr, 
> ##D             ylim = yr,
> ##D     legend.width = 1.5,
> ##D       legend.mar = 3,
> ##D      legend.args = list( text = "z",
> ##D                           cex = .8,
> ##D                          side = 3,
> ##D                          line = .5)
> ##D    )
> ##D 
> ## End(Not run)
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("image.plot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("image.smooth")
> ### * image.smooth
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: image.smooth
> ### Title: Kernel smoother for irregular 2-d data
> ### Aliases: image.smooth setup.image.smooth
> ### Keywords: smooth
> 
> ### ** Examples
> 
> # first convert precip data to the 128X128 discretized image format ( with 
> # missing  values to indicate where data is not observed) 
> # 
> out<- as.image( RMprecip$y, x= RMprecip$x, nx=128, ny=128) 
> # out$z is the image matrix 
> 
> dx<- out$x[2]- out$x[1] 
> dy<-  out$y[2] - out$y[1] 
> 
> #  
> # grid scale in degrees and choose kernel bandwidth to be .25 degrees. 
> 
> look<- image.smooth( out, aRange= .25)
> 
> # pass in a tophat kernel
> topHat<- function( dd, h ){ ifelse( dd <= h^2, 1, 0)} 
> ## dd is the distance squared
> look2<- image.smooth( out, kernel.function=topHat, h=.8)
> 
> image.plot(look) 
> points( RMprecip$x)
> US( add=TRUE, col="grey", lwd=2)
> 
> # to save on computation, decrease the padding with zeroes 
> # only pad 32 grid points around the margins ofthe image. 
> 
> look<- image.smooth(out$z, dx=dx, dy=dy, aRange= .25, xwidth=32*dx,ywidth=32*dy) 
> 
> # the range of these data is ~ 10 degrees  and so 
> # with a padding of 32 grid points  32*( 10/128) =  2.5 
> # about 10 standard deviations of the normal kernel so there is still 
> # lots of room for padding  
> # a minimal choice might be  xwidth = 4*(.25)= 1  4 SD for the normal kernel
> # creating weighting object outside the call  
> # this is useful when one wants to smooth different data sets but on the 
> # same grid with the same kernel function 
> # 
> 
> #
> #  random fields from smoothing white noise with this filter.
> #
> set.seed(123)
> test.image<- matrix( rnorm(128**2),128,128)
> dx<- .1
> dy<- .8
> 
> wght<- setup.image.smooth( nrow=128, ncol=128,  dx=dx, dy=dy,
+              aRange=.25, xwidth=2.5, ywidth=2.5)
> #
> look<- image.smooth( test.image, dx=dx, dy=dy, wght)
> 
> # NOTE:   this is the same as using 
> #
> #     image.smooth( test.image , 128,128), xwidth=2.5,
> #                        ywidth=2.5, dx=dx,dy=dy, aRange=.25)
> #
> #   but the call to image.smooth is faster because the fft of kernel
> #   has been precomputed.
> 
> 
> 
> # periodic smoothing in the horizontal dimension
> 
> look<- image.smooth( test.image , xwidth=1.5,
+                         ywidth=2.5, dx=dx,dy=dy, aRange=1.5)
> look2<- image.smooth( test.image , xwidth=0,
+                         ywidth=2.5, dx=dx,dy=dy, aRange=1.5)
> # compare these two
> set.panel( 1,2)
plot window will lay out plots in a 1 by 2 matrix 
> image.plot( look, legend.mar=7.1)
> title("free boundaries")
> image.plot( look2, legend.mar=7.1) # look for periodic continuity at edges!
> title("periodic boundary in horizontal")
> set.panel(1,1)
plot window will lay out plots in a 1 by 1 matrix 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("image.smooth", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("image2lz")
> ### * image2lz
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: image2lz
> ### Title: Some simple functions for subsetting images
> ### Aliases: image2lz crop.image in.poly in.poly.grid half.image
> ###   get.rectangle average.image which.max.matrix which.max.image
> ### Keywords: hplot
> 
> ### ** Examples
> 
> data(RMelevation)
> 
> # region defining Colorado Front Range
> 
>   loc<- rbind( c(-106.5, 40.8),
+              c(-103.9, 37.5))
> 
> # extract elevations for just CO frontrange.
>    FR<- crop.image(RMelevation, loc)
>    image.plot( FR, col=terrain.colors(256))
>    
>    which.max.image( FR)
$x
[1] -106.25

$y
[1] 38.625

$z
[1] 3975.202

$ind
     ix iy
[1,]  8 29

> 
> # average cells  4 to 1 by doing this twice!
>    temp<-  half.image( RMelevation)
>    temp<- half.image( temp)
> 
> # or in one step
>    temp<-  average.image( RMelevation, Q=4)-> temp
>    image.plot( temp, col=terrain.colors(256))
> 
> # a polygon (no special meaning entered with just locator)
> x1p<- c(
+  -106.2017, -104.2418, -102.9182, -102.8163, -102.8927, -103.3254, -104.7763,
+  -106.5581, -108.2889, -109.1035, -109.3325, -108.7980)
> 
> x2p<- c(
+   43.02978, 42.80732, 41.89727, 40.84566, 39.81427, 38.17618, 36.53810, 36.29542,
+   36.90211, 38.29752, 39.45025, 41.02767)
> xp<- cbind( x1p,x2p)
> 
>  image.plot( temp)
>  polygon( xp[,1], xp[,2], lwd=2)
> 
> # find all grid points inside poly
>  fullset<- make.surface.grid( list( x= temp$x, y= temp$y))
>  ind<-  in.poly( fullset,xp)
> 
> # take a look 
>  plot( fullset, pch=".")
>  polygon( xp[,1], xp[,2], lwd=2)
>  points( fullset[ind,], pch="o", col="red", cex=.5)
> 
> # masking out the image NA == white in the image plot
>  temp$z[!ind] <- NA
>  image.plot( temp)
>  polygon( xp[,1], xp[,2], lwd=2)
> 
> # This is more efficient for large grids:
> # because the large number of grid location ( xg above) is 
> # never explicitly created.
> 
>  ind<- in.poly.grid( list( x= temp$x, y= temp$y), xp)
> 
> # now use ind in the same way as above to mask points outside of polygon
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("image2lz", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("interp.surface")
> ### * interp.surface
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: interp.surface
> ### Title: Fast bilinear interpolator from a grid.
> ### Aliases: interp.surface interp.surface.grid
> ### Keywords: spatial
> 
> ### ** Examples
> 
> #
> # evaluate an image at a finer grid
> # 
> 
> data( lennon)
> # create an example in the right list format like image or contour
> obj<- list( x= 1:20, y=1:20, z= lennon[ 201:220, 201:220])
> 
> set.seed( 123)
> # lots of random points
> N<- 500
> loc<- cbind( runif(N)*20, runif(N)*20)
> z.new<- interp.surface( obj, loc)
> # compare the image with bilinear interpolation at scattered points
> set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
> image.plot( obj)
> quilt.plot( loc, z.new) 
> 
> 
> # sample at 100X100 equally spaced points on a grid
> 
> grid.list<- list( x= seq( 1,20,,100), y=  seq( 1,20,,100))
> 
> interp.surface.grid( obj, grid.list)-> look
> 
> # take a look
> set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
> image.plot( obj)
> image.plot( look)
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("interp.surface", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("mKrig")
> ### * mKrig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: mKrig
> ### Title: "micro Krig" Spatial process estimate of a curve or surface,
> ###   "kriging" with a known covariance function.
> ### Aliases: mKrig predict.mKrig mKrig.coef mKrig.trace print.mKrig
> ###   print.mKrigSummary summary.mKrig mKrigCheckXY
> ### Keywords: spatial
> 
> ### ** Examples
> 
> #
> # Midwest ozone data  'day 16' stripped of missings 
>   data( ozone2)
>   y<- ozone2$y[16,]
>   good<- !is.na( y)
>   y<-y[good]
>   x<- ozone2$lon.lat[good,]
> # nearly interpolate using defaults (Exponential covariance range = 2.0)
> # see also mKrigMLEGrid to choose lambda by maxmimum likelihood
>   out<- mKrig( x,y, aRange = 2.0, lambda=.01)
>   out.p<- predictSurface( out)
>   surface( out.p)
> #
> # NOTE this should be identical to 
> # Krig( x,y, aRange=2.0, lambda=.01) 
> 
> ##############################################################################
> # an example using a "Z" covariate and the Matern family
> #  again see mKrigMLEGrid to choose parameters by MLE.
> data(COmonthlyMet)
> yCO<- CO.tmin.MAM.climate
> good<- !is.na( yCO)
> yCO<-yCO[good]
> xCO<- CO.loc[good,]
> Z<- CO.elev[good]
> out<- mKrig( xCO,yCO, Z=Z, cov.function="stationary.cov", Covariance="Matern",
+                     aRange=4.0, smoothness=1.0, lambda=.1)
> set.panel(2,1)
plot window will lay out plots in a 2 by 1 matrix 
> # quilt.plot with elevations
> quilt.plot( xCO, predict(out))
> # Smooth surface without elevation linear term included
> surface( out)
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> 
> 
> #########################################################################
> # here is a series of examples with bigger datasets  
> # using a compactly supported covariance directly
> 
> set.seed( 334)
> N<- 1000
> x<- matrix( 2*(runif(2*N)-.5),ncol=2)
> y<- sin( 1.8*pi*x[,1])*sin( 2.5*pi*x[,2]) + rnorm( 1000)*.1
>   
> look2<-mKrig( x,y, cov.function="wendland.cov",k=2, aRange=.2, 
+             lambda=.1)
> 
> # take a look at fitted surface
> predictSurface(look2)-> out.p
> surface( out.p)
> 
> # this works because the number of nonzero elements within distance aRange
> # are less than the default maximum allocated size of the 
> # sparse covariance matrix. 
> #  see  options() for the default values. The names follow the convention
> # spam.arg where arg is the name of the spam component 
> #   e.g. spam.nearestdistnnz
> 
> # The following will give a warning for aRange=.9 because 
> # allocation for the  covariance matirx storage is too small. 
> # Here aRange controls the support of the covariance and so 
> # indirectly the  number of nonzero elements in the sparse matrix
> 
> ## Not run: 
> ##D  look2<- mKrig( x,y, cov.function="wendland.cov",k=2, aRange=.9, lambda=.1)
> ## End(Not run)
> 
> # The warning resets the memory allocation  for the covariance matrix
> # according the to values   options(spam.nearestdistnnz=c(416052,400))'
> # this is inefficient becuase the preliminary pass failed. 
> 
> # the following call completes the computation in "one pass"
> # without a warning and without having to reallocate more memory. 
> 
> options( spam.nearestdistnnz=c(416052,400))
>   look2<- mKrig( x,y, cov.function="wendland.cov",k=2,
+                     aRange=.9, lambda=1e-2)
> # as a check notice that 
> #   print( look2)
> # reports the number of nonzero elements consistent with the specifc allocation
> # increase in spam.options
> 
> 
> # new data set of 1500 locations
>   set.seed( 234)
>   N<- 1500
>   x<- matrix( 2*(runif(2*N)-.5),ncol=2)
>   y<- sin( 1.8*pi*x[,1])*sin( 2.5*pi*x[,2]) + rnorm( N)*.01
> 
> ## Not run: 
> ##D   
> ##D # the following is an example of where the allocation  (for nnzR) 
> ##D # for the cholesky factor is too small. A warning is issued and 
> ##D # the allocation is increased by 25##D 
> ##D #
> ##D  look2<- mKrig( x,y, 
> ##D             cov.function="wendland.cov",k=2, aRange=.1, lambda=1e2  )
> ## End(Not run)
> # to avoid the warning 
>  look2<-mKrig( x,y, 
+             cov.function="wendland.cov", k=2, aRange=.1, 
+             lambda=1e2, chol.args=list(pivot=TRUE, memory=list(nnzR= 450000)))
> 
> ###############################################################################
> # fiting multiple data sets
> #
> #\dontrun{ 
>   y1<- sin( 1.8*pi*x[,1])*sin( 2.5*pi*x[,2]) + rnorm( N)*.01
>   y2<- sin( 1.8*pi*x[,1])*sin( 2.5*pi*x[,2]) + rnorm( N)*.01
>   Y<- cbind(y1,y2)
>   look3<- mKrig( x,Y,cov.function="wendland.cov",k=2, aRange=.1, 
+             lambda=1e2  )
> # note slight difference in summary because two data sets have been fit.
>   print( look3)
[1]  0.015025061 -0.002809635 -0.026186902
Call:
mKrig(x = x, y = Y, cov.function = "wendland.cov", lambda = 100, 
    k = 2, aRange = 0.1)
                                                      
 Number of Locations:                           1500  
 Number of data sets fit:                       2     
 Degree of polynomial null space ( base model): 1     
 Total number of parameters in base model       3     
  Estimate Eff. degrees of freedom              19.21 
     Standard Error of Eff. Df                  0.5761
 Smoothing parameter                            100   
 Nonzero entries in covariance                  18614 
 
 
Summary of fixed effects
   estimate       SE pValue
d1  0.01503 0.009536 0.1151
d2 -0.00281 0.016650 0.8660
d3 -0.02619 0.016300 0.1082
 
Covariance Model: wendland.cov
   Non-default covariance arguments and their values 
   Argument: k  has the value(s): 
[1] 2
   Argument: aRange  has the value(s): 
[1] 0.1
> #}
> 
> ## Not run: 
> ##D ##################################################################
> ##D # finding a good choice for aRange as a taper 
> ##D 
> ##D # Suppose the target is a spatial prediction using roughly 50 nearest neighbors
> ##D # (tapering covariances is effective for roughly 20 or more in the situation of 
> ##D #  interpolation) see Furrer, Genton and Nychka (2006).
> ##D # take a look at a random set of 100 points to get idea of scale
> ##D # and saving  computation time by not  looking at the complete set
> ##D # of points
> ##D # NOTE: This could also be done directly using the  FNN package for finding nearest 
> ##D # neighbors
> ##D   set.seed(223)
> ##D   ind<- sample( 1:N,100)
> ##D   hold<- rdist( x[ind,], x)
> ##D   dd<- apply( hold, 1, quantile, p= 50/N )
> ##D   dguess<- max(dd)
> ##D # dguess is now a reasonable guess at finding cutoff distance for
> ##D # 50 or so neighbors
> ##D # full distance matrix excluding distances greater than dguess
> ##D   hold2<- nearest.dist( x, x, delta= dguess )
> ##D # here is trick to find the number of nonsero rows for a matrix in spam format. 
> ##D   hold3<-  diff( hold2@rowpointers)
> ##D #  min( hold3) = 43   which we declare close enough. This also counts the diagonal
> ##D # So there are a minimum of 42 nearest neighbors  ( median is 136)
> ##D # see  table( hold3) for the distribution 
> ##D # now the following will use no less than 43 - 1  nearest neighbors 
> ##D # due to the tapering. 
> ##D 
> ##D   mKrig( x,y, cov.function="wendland.cov",k=2, aRange=dguess, 
> ##D             lambda=1e2) ->  look2
> ## End(Not run)
> 
> ###############################################################################
> # use precomputed distance matrix
> #
> ## Not run: 
> ##D  
> ##D   y1<- sin( 1.8*pi*x[,1])*sin( 2.5*pi*x[,2]) + rnorm( N)*.01
> ##D   y2<- sin( 1.8*pi*x[,1])*sin( 2.5*pi*x[,2]) + rnorm( N)*.01
> ##D   Y<- cbind(y1,y2)
> ##D   #precompute distance matrix in compact form
> ##D   distMat = rdist(x, compact=TRUE)
> ##D   look3<- mKrig( x,Y,cov.function="stationary.cov", aRange=.1, 
> ##D             lambda=1e2, distMat=distMat )
> ##D   #precompute distance matrix in standard form
> ##D   distMat = rdist(x)
> ##D   look3<- mKrig( x,Y,cov.function="stationary.cov", aRange=.1, 
> ##D             lambda=1e2, distMat=distMat )
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("mKrig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("mKrigMLE")
> ### * mKrigMLE
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: mKrigMLE
> ### Title: Maximizes likelihood for the process marginal variance (sigma)
> ###   and nugget standard deviation (tau) parameters (e.g. lambda) over a
> ###   many covariance models or covariance parameter values.
> ### Aliases: mKrigMLEJoint mKrigMLEGrid mKrigJointTemp.fn profileCI
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D #perform joint likelihood maximization over lambda and aRange. 
> ##D # NOTE: optim can get a bad answer with poor initial starts.
> ##D   data(ozone2)
> ##D   s<- ozone2$lon.lat
> ##D   z<- ozone2$y[16,]
> ##D   gridList<- list( aRange = seq( .4,1.0,length.out=20),
> ##D                  lambda = 10**seq( -1.5,0,length.out=20)
> ##D                  )
> ##D   par.grid<- make.surface.grid( gridList)
> ##D   out<- mKrigMLEGrid( s,z, par.grid=par.grid,
> ##D                           cov.args= list(smoothness=1.0,
> ##D                                      Covariance="Matern" )
> ##D                           )   
> ##D   outP<- as.surface( par.grid, out$summary[,"lnProfileLike.FULL"])
> ##D   image.plot( outP$x, log10(outP$y),outP$z,
> ##D                xlab="aRange", ylab="log10 lambda")
> ##D                
> ## End(Not run)
>  
>  ## Not run: 
> ##D   N<- 50
> ##D   set.seed(123)
> ##D   x<- matrix(runif(2*N), N,2)
> ##D   aRange<- .2
> ##D   Sigma<-  Matern( rdist(x,x)/aRange , smoothness=1.0)
> ##D   Sigma.5<- chol( Sigma)
> ##D   tau<- .1
> ##D   #  250 independent spatial data sets but a common covariance function 
> ##D   #    -- there is little overhead in
> ##D   #        MLE across independent realizations and a good test of code validity.
> ##D   M<-250
> ##D   F.true<- t( Sigma.5) %*% matrix( rnorm(N*M), N,M)
> ##D   Y<-  F.true +  tau* matrix( rnorm(N*M), N,M)
> ##D 
> ##D # find MLE for lambda with grid of ranges 
> ##D # and smoothness fixed in Matern                     
> ##D  par.grid<- list( aRange= seq( .1,.35,,8))
> ##D   obj1b<- mKrigMLEGrid( x,Y,
> ##D      cov.args = list(Covariance="Matern", smoothness=1.0), 
> ##D         par.grid = par.grid
> ##D                     )
> ##D   obj1b$summary # take a look
> ##D # profile over aRange
> ##D   plot( par.grid$aRange, obj1b$summary[,"lnProfileLike.FULL"],
> ##D     type="b", log="x")
> ##D  
> ## End(Not run)
>   ## Not run: 
> ##D # m=0 is a simple switch to indicate _no_ fixed spatial drift
> ##D # (the default and highly recommended  is linear drift, m=2). 
> ##D # However, m=0 results in MLEs that are less biased, being the correct model
> ##D # -- in fact it nails it !
> ##D 
> ##D   obj1a<- mKrigMLEJoint(x,Y, 
> ##D                     cov.args=list(Covariance="Matern", smoothness=1.0), 
> ##D                     cov.params.start=list(aRange =.5, lambda = .5),
> ##D                      mKrig.args= list( m=0))
> ##D  
> ##D  test.for.zero( obj1a$summary["tau"], tau, tol=.007)
> ##D  test.for.zero( obj1a$summary["aRange"], aRange, tol=.015)
> ##D  
> ## End(Not run) 
> 
> 
> ##########################################################################
> # A bootstrap example
> # Here is a example of a more efficient (but less robust) bootstrap using 
> # mKrigMLEJoint and tuned starting values
> ##########################################################################
> ## Not run: 
> ##D data( ozone2)
> ##D obj<- spatialProcess( ozone2$lon.lat,ozone2$y[16,] )
> ##D 
> ##D ######### boot strap 
> ##D   set.seed(123)
> ##D   M<- 250
> ##D # create M indepedent copies of the observation vector
> ##D   ySynthetic<- simSpatialData( obj, M)
> ##D   bootSummary<- NULL
> ##D   
> ##D  aRangeMLE<- obj$summary["aRange"]
> ##D  lambdaMLE<- obj$summary["lambda"]
> ##D  
> ##D   for(  k in 1:M){
> ##D   cat( k, " " )
> ##D # here the MLEs are found using the easy top level level wrapper
> ##D # see mKrigMLEJoint for a more efficient strategy
> ##D   out <- mKrigMLEJoint(obj$x, ySynthetic[,k],
> ##D                  weights = obj$weights,
> ##D               mKrig.args = obj$mKrig.args,
> ##D                  cov.function = obj$cov.function.name,
> ##D                 cov.args = obj$cov.args, 
> ##D         cov.params.start = list( aRange = aRangeMLE,
> ##D                                 lambda = lambdaMLE)
> ##D                       )
> ##D   newSummary<- out$summary
> ##D   bootSummary<- rbind( bootSummary, newSummary)
> ##D   }
> ##D   
> ##D   cat(  " ", fill=TRUE )
> ##D   
> ##D   obj$summary
> ##D   stats( bootSummary)
> ##D   
> ## End(Not run)
> ## Not run: 
> ##D #perform joint likelihood maximization over lambda, aRange, and smoothness.  
> ##D #note: finding smoothness is not a robust optimiztion 
> ##D #      can get a bad answer with poor initial guesses.
> ##D obj2<- mKrigMLEJoint(x,Y, 
> ##D                       cov.args=list(Covariance="Matern"), 
> ##D                       cov.params.start=list( aRange = .18,
> ##D                                          smoothness = 1.1,
> ##D                                              lambda = .08),
> ##D                        )
> ##D 
> ##D #look at lnLikelihood  evaluations
> ##D obj2$summary
> ##D #compare to REML
> ##D obj3<- mKrigMLEJoint(x,Y, 
> ##D                       cov.args=list(Covariance="Matern"), 
> ##D                       cov.params.start=list(aRange = .18, 
> ##D                                        smoothness = 1.1,
> ##D                                            lambda = .08),
> ##D                        , REML=TRUE)
> ##D obj3$summary                      
> ## End(Not run)
> ## Not run: 
> ##D #look at lnLikelihood  evaluations
> ##D 
> ##D # check convergence of MLE to true fit with no fixed part
> ##D # 
> ##D obj4<- mKrigMLEJoint(x,Y, 
> ##D                       mKrig.args= list( m=0),
> ##D                       cov.args=list(Covariance="Matern", smoothness=1),
> ##D                       cov.params.start=list(aRange=.2, lambda=.1),
> ##D                        REML=TRUE)
> ##D #look at lnLikelihood  evaluations
> ##D obj4$summary
> ##D # nails it!
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("mKrigMLE", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("ozone")
> ### * ozone
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: Chicago ozone test data
> ### Title: Data set of ozone measurements at 20 Chicago monitoring
> ###   stations.
> ### Aliases: ChicagoO3 ozone
> ### Keywords: datasets
> 
> ### ** Examples
> 
> fit<- Tps(ChicagoO3$x, ChicagoO3$y) 
> # fitting a surface to ozone measurements. 
> surface( fit, type="I")
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("ozone", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("ozone2")
> ### * ozone2
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: ozone2
> ### Title: Daily 8-hour ozone averages for sites in the Midwest
> ### Aliases: ozone2
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data( ozone2)
> 
> # pairwise correlation among all stations
> # ( See cover.design to continue this example)
> cor.mat<- cor( ozone2$y, use="pairwise")
> 
> #raw data image for day number 16 
> good<- !is.na( ozone2$y[16,])
> out<- as.image( ozone2$y[16,good], x=ozone2$lon.lat[good,])
> image.plot( out)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("ozone2", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("plot.Krig")
> ### * plot.Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: plot.Krig
> ### Title: Diagnostic and summary plots of a Kriging, spatialProcess or
> ###   spline object.
> ### Aliases: plot.Krig plot.sreg
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> data( ozone2)
> x<- ozone2$lon.lat
> y<- ozone2$y[16,]
> fit1<-Krig(x,y, aRange=200)  
> # fitting a surface to ozone  
> # measurements 
> set.panel( 2,2)
plot window will lay out plots in a 2 by 2 matrix 
> plot(fit1)
> 
> # fit rat data
> fit3<-sreg(rat.diet$t,rat.diet$con)
> set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
> plot(fit3)       
> 
> set.panel(1,1) # reset graphics window. 
plot window will lay out plots in a 1 by 1 matrix 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("plot.Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("plot.surface")
> ### * plot.surface
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: plot.surface
> ### Title: Plots a surface
> ### Aliases: plot.surface
> ### Keywords: hplot
> 
> ### ** Examples
> 
> x<- seq( -2,2,,80)
> y<- seq( -2,2,,80)
> # a lazy way to create some test image
> z<- outer( x,y, "+")
> 
> # create basic image/surface object
> obj<- list(x=x, y=y,z=z)
> 
> # basic contour plot
> # note how graphical parameters appropriate to contour are passed
> plot.surface( obj, type="c", col="red")
> 
> # using a fields function to fit a surface and evaluate as surface object.
> fit<- Tps( BD[,1:4], BD$lnya) # fit surface to data 
> # surface of variables 2 and 3 holding 1 and 4 fixed at their median levels
>  out.p<-predictSurface(fit, xy=c(2,3))  
> 
>  plot.surface(out.p) # surface plot  
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("plot.surface", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("poly.image")
> ### * poly.image
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: poly.image
> ### Title: Image plot for cells that are irregular quadrilaterals.
> ### Aliases: poly.image poly.image.regrid
> ### Keywords: spatial
> 
> ### ** Examples
> 
> data(RCMexample)
> set.panel( 1,2)
plot window will lay out plots in a 1 by 2 matrix 
> par(pty="s")
> # plot with grid modified
> poly.image( RCMexample$x, RCMexample$y, RCMexample$z[,,1])
> 
> # use midpoints of z
> poly.image( RCMexample$x, RCMexample$y, RCMexample$z[,,1],midpoint=TRUE)
> 
>   set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> # an example with quantile breaks
> 
>  brk<- quantile(  RCMexample$z[,,1], c( 0, .9,.95,.99,1.0) )
>  poly.image( RCMexample$x, RCMexample$y, RCMexample$z[,,1], breaks=brk, col=
+     rainbow(4))
>   
> 
> # images are very similar. 
>   set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> # Regridding of x and y
>   l1<- poly.image.regrid( RCMexample$x)
>   l2<- poly.image.regrid( RCMexample$y)
> 
> # test that this works
>   i<- 1:10
>   plot( l1[i,i], l2[i,i])
>   points( RCMexample$x[i,i], RCMexample$y[i,i],col="red")
> 
>  
> 
>   
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("poly.image", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("predict.Krig")
> ### * predict.Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: predict.Krig
> ### Title: Evaluation of Krig spatial process estimate.
> ### Aliases: predict.Krig predict.Tps predictDerivative.Krig
> ###   predict.fastTps
> ### Keywords: spatial
> 
> ### ** Examples
> 
>   Krig(ChicagoO3$x,ChicagoO3$y, aRange=50) ->fit
>   predict( fit) # gives predicted values at data points should agree with fitted.values
          [,1]
 [1,] 38.19526
 [2,] 38.68851
 [3,] 38.04500
 [4,] 38.73050
 [5,] 38.90710
 [6,] 39.89087
 [7,] 38.38276
 [8,] 39.45331
 [9,] 39.73383
[10,] 39.65974
[11,] 40.90296
[12,] 40.16104
[13,] 41.00987
[14,] 40.66807
[15,] 41.22062
[16,] 40.50951
[17,] 39.72912
[18,] 39.32462
[19,] 41.29463
[20,] 41.10218
>                 #  in fit object 
> 
> # predict at the coordinate (-5,10)
>   x0<- cbind( -5,10) # has to be a  1X2 matrix
>   predict( fit,x= x0)
         [,1]
[1,] 39.30373
> 
> # redoing predictions at data locations:
>    predict( fit, x=ChicagoO3$x)
          [,1]
 [1,] 38.19526
 [2,] 38.68851
 [3,] 38.04500
 [4,] 38.73050
 [5,] 38.90710
 [6,] 39.89087
 [7,] 38.38276
 [8,] 39.45331
 [9,] 39.73383
[10,] 39.65974
[11,] 40.90296
[12,] 40.16104
[13,] 41.00987
[14,] 40.66807
[15,] 41.22062
[16,] 40.50951
[17,] 39.72912
[18,] 39.32462
[19,] 41.29463
[20,] 41.10218
> 
> # only the fixed part of the model
>   predict( fit, just.fixed=TRUE) 
          [,1]
 [1,] 40.83480
 [2,] 40.96135
 [3,] 40.80576
 [4,] 40.75224
 [5,] 40.96245
 [6,] 40.77402
 [7,] 40.88791
 [8,] 40.99245
 [9,] 41.01029
[10,] 40.85702
[11,] 40.99142
[12,] 41.07368
[13,] 41.19898
[14,] 41.13733
[15,] 41.09190
[16,] 40.72847
[17,] 40.74733
[18,] 40.76694
[19,] 40.77036
[20,] 40.75996
> 
> # evaluating estimate at a grid of points 
>   grid<- make.surface.grid( list( seq( -40,40,,15), seq( -40,40,,15)))
>   look<- predict(fit,grid) # evaluate on a grid of points
> 
> # some useful graphing functions for these gridded predicted values
>   out.p<- as.surface( grid, look) # reformat into $x $y $z image-type object
>   contour( out.p) 
> 
> # see also the functions predictSurface and surface 
> # for functions that combine these steps 
>    
> 
> # refit with 10 degrees of freedom in surface
>   look<- predict(fit,grid, df=15)
> # refit with random data 
>   look<- predict( fit, grid, y= rnorm( 20))
> 
> 
> # finding partial derivatives of the estimate
> #
> # find the partial derivatives at observation locations
> # returned object is a two column matrix. 
> # this does not make sense for the exponential covariance
> # but can illustrate this with a thin plate spline with
> # a high enough order ( i.e. need m=3 or greater)
> # 
>   data(ozone2)
> # the 16th day of this ozone spatial dataset
>   fit0<- Tps( ozone2$lon.lat, ozone2$y[16,], m=3)
>   look1<- predictDerivative.Krig( fit0)
> # for extra credit compare this to
>   look2<- predictDerivative.Krig( fit0, x=ozone2$lon.lat)  
> # (why are there more values in look2) 
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("predict.Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("predictSE.Krig")
> ### * predictSE.Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: predictSE
> ### Title: Standard errors of predictions for Krig spatial process estimate
> ### Aliases: predictSE predictSE.Krig predictSE.mKrig predictSEUsingKrigA
> ### Keywords: spatial
> 
> ### ** Examples
> 
> # 
> # Note: in these examples predictSE will default to predictSE.Krig using 
> # a Krig object  
> 
>   fit<- Krig(ChicagoO3$x,ChicagoO3$y,cov.function="Exp.cov", aRange=10)    # Krig fit 
>   predictSE.Krig(fit)      # std errors of predictions at obs.
 [1] 1.605075 1.625888 1.582081 1.717259 1.553386 2.037997 1.588340 1.624983
 [9] 1.753248 1.970771 2.139202 1.804267 2.430791 2.011470 2.115035 2.388593
[17] 1.871492 1.662702 2.323701 2.039209
> 
> # make a  grid of X's  
>   xg<-make.surface.grid( 
+   list(East.West=seq(-27,34,,20),North.South=seq(-20,35,,20)))     
>   out<- predictSE(fit,xg)   # std errors of predictions 
> 
> #at the grid points out is a vector of length 400 
> #reshape the grid points into a 20X20 matrix etc.  
> 
>    out.p<-as.surface( xg, out) 
>    surface( out.p, type="C") 
> 
> # this is equivalent to  the single step function  
> # (but default is not to extrapolation beyond data
> # out<- predictSurfaceSE( fit) 
> # image.plot( out) 
> 
> 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("predictSE.Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("predictSurface")
> ### * predictSurface
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: predictSurface
> ### Title: Evaluates a fitted function or the prediction error as a surface
> ###   that is suitable for plotting with the image, persp, or contour
> ###   functions.
> ### Aliases: predictSurface predictSurface.default predictSurface.mKrig
> ###   predictSurface.Krig predictSurface.fastTps predictSurfaceSE
> ###   predictSurfaceSE.default predict.surface
> ### Keywords: spatial
> 
> ### ** Examples
> 
> fit<- Tps( BD[,1:4], BD$lnya)  # fit surface to data 
> 
> # evaluate fitted surface for  first two 
> # variables holding other two fixed at median values
> 
> out.p<- predictSurface(fit)
> surface(out.p, type="C") 
> 
> #
> # plot surface for second and fourth variables 
> # on specific grid. 
> 
> glist<- list( KCL=29.77, MgCl2= seq(3,7,,25), KPO4=32.13, 
+                      dNTP=seq( 250,1500,,25))
> 
> out.p<- predictSurface(fit, glist)
> surface(out.p, type="C")
> 
> out.p<- predictSurfaceSE(fit, glist)
> surface(out.p, type="C")
>   
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("predictSurface", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("print.Krig")
> ### * print.Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: print.Krig
> ### Title: Print kriging fit results.
> ### Aliases: print.Krig
> ### Keywords: spatial
> 
> ### ** Examples
> 
> fit<- Krig(ChicagoO3$x,ChicagoO3$y, aRange=100) 
> print(fit) # print the summary 
Call:
Krig(x = ChicagoO3$x, Y = ChicagoO3$y, aRange = 100)
                                             
 Number of Observations:                20   
 Number of parameters in the null space 3    
 Parameters for fixed spatial drift     3    
 Model degrees of freedom:              5.4  
 Residual degrees of freedom:           14.6 
 GCV estimate for tau:                  4.012
 MLE for tau:                           3.699
 MLE for sigma:                         20.25
 lambda                                 0.68 
 User supplied sigma                    NA   
 User supplied tau^2                    NA   
Summary of estimates: 
              lambda      trA      GCV   tauHat -lnLike Prof converge
GCV        0.9654031 4.842326 22.02399 4.085538     49.16244        4
GCV.model         NA       NA       NA       NA           NA       NA
GCV.one    0.9654031 4.842326 22.02399 4.085538           NA        4
RMSE              NA       NA       NA       NA           NA       NA
pure error        NA       NA       NA       NA           NA       NA
REML       0.6755001 5.442135 22.11055 4.011747     49.14736        3
> fit # this will work too 
Call:
Krig(x = ChicagoO3$x, Y = ChicagoO3$y, aRange = 100)
                                             
 Number of Observations:                20   
 Number of parameters in the null space 3    
 Parameters for fixed spatial drift     3    
 Model degrees of freedom:              5.4  
 Residual degrees of freedom:           14.6 
 GCV estimate for tau:                  4.012
 MLE for tau:                           3.699
 MLE for sigma:                         20.25
 lambda                                 0.68 
 User supplied sigma                    NA   
 User supplied tau^2                    NA   
Summary of estimates: 
              lambda      trA      GCV   tauHat -lnLike Prof converge
GCV        0.9654031 4.842326 22.02399 4.085538     49.16244        4
GCV.model         NA       NA       NA       NA           NA       NA
GCV.one    0.9654031 4.842326 22.02399 4.085538           NA        4
RMSE              NA       NA       NA       NA           NA       NA
pure error        NA       NA       NA       NA           NA       NA
REML       0.6755001 5.442135 22.11055 4.011747     49.14736        3
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("print.Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("pushpin")
> ### * pushpin
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: pushpin
> ### Title: Adds a "push pin" to an existing 3-d plot
> ### Aliases: pushpin
> ### Keywords: hplot
> 
> ### ** Examples
> 
> # Dr. R's favorite New  Zealand Volcano!
>      data( volcano)
>      M<- nrow( volcano)
>      N<- ncol( volcano)
>      x<- seq( 0,1,,M)
>      y<- seq( 0,1,,N)
> 
>      drape.plot( x,y,volcano, col=terrain.colors(128))-> pm 
> 
>      max( volcano)-> zsummit
>      xsummit<- x[ row( volcano)[volcano==zsummit]]
>      ysummit<- y[ col( volcano)[volcano==zsummit]]
> 
> pushpin( xsummit,ysummit,zsummit,pm, text="Summit")
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("pushpin", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("qsreg")
> ### * qsreg
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: qsreg
> ### Title: Quantile or Robust spline regression
> ### Aliases: qsreg
> ### Keywords: smooth
> 
> ### ** Examples
> 
> 
>      # fit a CV  quantile spline
>      fit50<- qsreg(rat.diet$t,rat.diet$con)
>      # (default is .5 so this is an estimate of the conditional median)
>      # control group of rats.
>      plot( fit50)
plot window will lay out plots in a 2 by 2 matrix 
>      predict( fit50)
 [1] 20.49998 20.40356 20.25621 19.84628 19.89900 20.14532 20.99036 21.50002
 [9] 22.79981 24.83986 25.14912 25.28184 24.40011 24.39903 24.83234 26.21044
[17] 26.64899 27.37894 27.55002 27.45862 27.25645 27.60002 27.74726 27.84999
[25] 27.72075 27.80000 27.98235 27.60001 27.44900 27.19902 27.79998 28.19900
[33] 27.99998 27.42413 27.89900 28.69814 28.57225 28.59999 27.50000
>      # predicted values at data points
>      xg<- seq(0,110,,50)
>      plot( fit50$x, fit50$y)
>      lines( xg, predict( fit50, xg))
> 
>      # A robust fit to rat diet data
>      # 
>      SC<- .5* median(abs((rat.diet$con- median(rat.diet$con))))
>      fit.robust<- qsreg(rat.diet$t,rat.diet$con, sc= SC)
>      plot( fit.robust)
plot window will lay out plots in a 2 by 2 matrix 
> 
>      # The global GCV function suggests little smoothing so 
>      # try the local
>      # minima with largest lambda instead of this default value.
>      # one should should consider redoing the three quantile fits in this
>      # example after looking at the cv functions and choosing a good value for
>      #lambda
>      # for example
>      lam<- fit50$cv.grid[,1]
>      tr<- fit50$cv.grid[,2]
>      # lambda close to df=6
>      lambda.good<- max(lam[tr>=6])
>      fit50.subjective<-qsreg(rat.diet$t,rat.diet$con, lam= lambda.good)
>      fit10<-qsreg(rat.diet$t,rat.diet$con, alpha=.1, nstep.cv=200)
>      fit90<-qsreg(rat.diet$t,rat.diet$con, alpha=.9, nstep.cv=200)
>      # spline fits at 50 equally spaced points
>      sm<- cbind(
+  
+      predict( fit10, xg),
+      predict( fit50.subjective, xg),predict( fit50, xg),
+      predict( fit90, xg))
>  
>      # and now zee data ...
>      plot( fit50$x, fit50$y)
>      # and now zee quantile splines at 10% 50% and 90%.
>      #
>      matlines( xg, sm, col=c( 3,3,2,3), lty=1) # the spline
>   
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("qsreg", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("quilt.plot")
> ### * quilt.plot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: quilt.plot
> ### Title: Useful plots for visualizing irregular spatial data.
> ### Aliases: quilt.plot bubblePlot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
> data( ozone2)
> # plot 16 day of ozone data set
> 
> quilt.plot( ozone2$lon.lat, ozone2$y[16,])
> US( add=TRUE, col="grey", lwd=2)
> 
> bubblePlot( ozone2$lon.lat, ozone2$y[16,]
+ )
> US( add=TRUE, col="grey", lwd=2)
> 
> 
> ### adding a common legend strip "by hand"
> ## to a panel of plots 
> ## and a custom color table
> 
> coltab<- two.colors( 256, middle="grey50" )
> 
> par( oma=c( 0,0,0,5)) # save some room for the legend
> set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
> zr<- range( ozone2$y, na.rm=TRUE)
> 
> for( k in 1:4){
+ quilt.plot( ozone2$lon.lat, ozone2$y[15+k,], add.legend=FALSE,
+  zlim=zr, col=coltab, nx=40, ny=40)
+ US( add=TRUE)
+ }
> par( oma=c(0,0,0,1))
> image.plot(zlim=zr,legend.only=TRUE, col=coltab)
> # may have to adjust number of spaces in oma to make this work.
>   
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("quilt.plot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("rdist")
> ### * rdist
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: rdist
> ### Title: Euclidean distance matrix or vector
> ### Aliases: rdist fields.rdist.near rdist.vec
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> out<- rdist( ChicagoO3$x)
> # out is a 20X20 matrix.
> 
> out2<- rdist( ChicagoO3$x[1:5,], ChicagoO3$x[11:20,])
> #out2 is a 5X10 matrix
> 
> set.seed(123)
> x1<- matrix( runif( 20*2), 20,2)
> x2<-  matrix( runif( 15*2), 15,2)
> 
> out3<- fields.rdist.near( x1,x2, delta=.5)
> # out3 is a sparse structure in list format
> 
> # or to "save"  work space decrease size of temp array
> 
>  out3<- fields.rdist.near( x1,x2, delta=.5,max.points=20*15)
> 
> # explicitly reforming as a full matrix 
> temp<- matrix( NA, nrow=out3$da[1], ncol= out3$da[2])
> temp[ out3$ind] <- out3$ra 
> 
> #       or justuse 
> 
>   temp<- spind2full( out3)
>   image( temp)
> 
> # this is  identical to 
>  temp2<- rdist( x1,x2)
>  temp2[ temp2<= .5] <- NA
> 
> #compute pairwise distance vector
> x1 = 1:10
> x2 = seq(from=10, to=1)
> rdist.vec(x1, x2)
 [1] 9 7 5 3 1 1 3 5 7 9
> 
> #calculate output matrix in compact form:
> distOut = rdist(1:10, compact=TRUE)
> distOut
   1 2 3 4 5 6 7 8 9
2  1                
3  2 1              
4  3 2 1            
5  4 3 2 1          
6  5 4 3 2 1        
7  6 5 4 3 2 1      
8  7 6 5 4 3 2 1    
9  8 7 6 5 4 3 2 1  
10 9 8 7 6 5 4 3 2 1
> as.vector(distOut)
 [1] 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 1 2 3 4 5 6 1 2 3 4 5 1 2 3
[39] 4 1 2 3 1 2 1
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("rdist", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("rdist.earth")
> ### * rdist.earth
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: rdist.earth
> ### Title: Great circle distance matrix or vector
> ### Aliases: rdist.earth rdist.earth.vec RdistEarth
> ### Keywords: spatial
> 
> ### ** Examples
> 
> data(ozone2)
> out<- rdist.earth ( ozone2$lon.lat)
> #out is a 153X153 distance matrix
> 
> out2<- RdistEarth ( ozone2$lon.lat)
> all.equal(out, out2)
[1] TRUE
> 
> upper<-  col(out)> row( out)
> # histogram of all pairwise distances. 
> hist( out[upper])
> 
> #get pairwise distances between first 10 and second 10 lon/lat points
> x1 = ozone2$lon.lat[1:10,]
> x2 = ozone2$lon.lat[11:20,]
> dists = rdist.earth.vec(x1, x2)
> print(dists)
 [1] 233.92528 137.13272  27.45679 206.58464  43.33655  31.13017  55.62943
 [8]  23.30296  40.37388 150.58339
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("rdist.earth", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("registeredC")
> ### * registeredC
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: registeringCode
> ### Title: Information objects that register C and FORTRAN functions.
> ### Aliases: addToDiagC ExponentialUpperC compactToMatC multebC
> ###   multwendlandg mltdrb RdistC distMatHaversin distMatHaversin2
> ### Keywords: datasets
> 
> ### ** Examples
> 
> print(addToDiagC)
$name
[1] "addToDiagC"

$address
<pointer: 0x7fbcf2f04e10>
attr(,"class")
[1] "RegisteredNativeSymbol"

$dll
DLL name: fields
Filename:
        /Users/nychka/Dropbox/Home/Repositories/fields/fields.Rcheck/fields/libs/fields.so
Dynamic lookup: FALSE

$numParameters
[1] 3

attr(,"class")
[1] "CallRoutine"      "NativeSymbolInfo"
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("registeredC", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("ribbon.plot")
> ### * ribbon.plot
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: ribbon.plot
> ### Title: Adds to an existing plot, a ribbon of color, based on values
> ###   from a color scale, along a sequence of line segments.
> ### Aliases: ribbon.plot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> plot( c(-1.5,1.5),c(-1.5,1.5), type="n")
> temp<- list( x= seq( -1,1,,40), y= seq( -1,1,,40))
> temp$z <- outer( temp$x, temp$y, "+")
> contour( temp, add=TRUE)
> 
> t<- seq( 0,.5,,50)
> y<- sin( 2*pi*t)
> x<- cos( pi*t)
> z<- x + y
> 
> ribbon.plot( x,y,z, lwd=10)
> 
> persp(  temp, phi=15, shade=.8, col="grey")-> pm
> trans3d( x,y,z,pm)-> uv
> ribbon.plot( uv$x, uv$y, z**2,lwd=5)
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("ribbon.plot", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("set.panel")
> ### * set.panel
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: set.panel
> ### Title: Specify a panel of plots
> ### Aliases: set.panel
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.panel(5,2) #divide screen to hold 10 plots where there are 5 rows  
plot window will lay out plots in a 5 by 2 matrix 
> 	       #and 2 columns 
> plot( 1:10) 
> plot( 2:8)
> 
> set.panel() #reset screen to one plot per screen 
plot window will lay out plots in a 1 by 1 matrix 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("set.panel", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("sim.Krig")
> ### * sim.Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: sim.spatialProcess
> ### Title: Conditional simulation of a spatial process
> ### Aliases: sim.Krig simSpatialData sim.spatialProcess sim.Krig.approx
> ###   sim.mKrig.approx
> ### Keywords: spatial
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ## A simple example for setting up a bootstrap 
> ##D ## M below should be
> ##D ## set to much larger sample size ( e.g. M <- 1000) for better
> ##D ## statistics
> ##D 
> ##D data( ozone2)
> ##D obj<- spatialProcess( ozone2$lon.lat,ozone2$y[16,] )
> ##D aHat<- obj$summary["aRange"]
> ##D lambdaHat<- obj$summary["lambda"]
> ##D ######### boot strap 
> ##D set.seed(123)
> ##D M<- 100
> ##D # create M indepedent copies of the observation vector
> ##D ySynthetic<- simSpatialData( obj, M)
> ##D 
> ##D bootSummary<- NULL
> ##D 
> ##D for(  k in 1:M){
> ##D cat( k, " ")
> ##D # here the MLEs are found using the easy top level level wrapper
> ##D # see mKrigMLEJoint for a more efficient strategy
> ##D   newSummary<- spatialProcess(obj$x,ySynthetic[,k],
> ##D                     cov.params.start= list(
> ##D 			                   aRange = aHat,
> ##D 			                  lambda = lambdaHat)
> ##D                                )$summary
> ##D   bootSummary<- rbind( bootSummary, newSummary)
> ##D   }
> ##D cat( fill= TRUE)
> ##D # the results and 95##D 
> ##D   stats( bootSummary )
> ##D 
> ##D   obj$summary
> ##D   tmpBoot<- bootSummary[,c("lambda", "aRange") ]
> ##D   confidenceInterval <- apply(tmpBoot, 2,
> ##D                                quantile, probs=c(0.025,0.975) )
> ##D # compare to estimates used as the "true" parameters			       
> ##D   obj$summary[2:5] 
> ##D   print( t(confidenceInterval) )
> ##D # compare to confidence interval using large sample theory  
> ##D   print( obj$CITable)
> ## End(Not run)
> 
> ## Not run: 
> ##D # conditional simulation with covariates
> ##D # colorado climate example
> ##D   data(COmonthlyMet)
> ##D   fit1E<- spatialProcess(CO.loc,CO.tmin.MAM.climate, Z=CO.elev   )
> ##D # conditional simulation at missing data
> ##D   good<- !is.na(CO.tmin.MAM.climate ) 
> ##D   infill<- sim.spatialProcess( fit1E, xp=CO.loc[!good,], 
> ##D                 Z= CO.elev[!good], M= 10)
> ##D # get an elevation grid  ... NGRID<- 50 gives a nicer image but takes longer 
> ##D  NGRID <- 25  
> ##D  # get elevations on a grid  
> ##D    COGrid<- list( x=seq( -109.5, -101, ,NGRID), y= seq(39, 41.5,,NGRID) )
> ##D    COGridPoints<- make.surface.grid( COGrid)
> ##D  # elevations are a bilinear interpolation from the 4km
> ##D  # Rocky Mountain elevation fields data set.   
> ##D    data( RMelevation)
> ##D    COElevGrid<- interp.surface( RMelevation, COGridPoints )
> ##D # NOTE call to sim.Krig treats the grid points as just a matrix
> ##D # of locations the plot has to "reshape" these into a grid 
> ##D # to use with image.plot 
> ##D    SEout<- sim.spatialProcess( fit1E, xp=COGridPoints,  Z= COElevGrid, M= 30)
> ##D # for just the smooth surface in lon/lat
> ##D #  SEout<- sim.spatialProcess( fit1E, xp=COGridPoints,  drop.Z=TRUE, M= 30)
> ##D # in practice M should be larger to reduce Monte Carlo error.      
> ##D    surSE<- apply( SEout, 2, sd )
> ##D    image.plot( as.surface( COGridPoints, surSE)) 
> ##D    points( fit1E$x, col="magenta", pch=16) 
> ##D    
> ## End(Not run)
> 
> data( ozone2)
> set.seed( 399)
> # fit to day 16 from Midwest ozone data set.
>   out<- Krig( ozone2$lon.lat, ozone2$y[16,], Covariance="Matern", 
+             aRange=1.0,smoothness=1.0, na.rm=TRUE)
> 
> # NOTE aRange =1.0 is not the best choice but 
> # allows the sim.rf circulant embedding algorithm to 
> # work without increasing the domain.
> 
> #six missing data locations
>  xp<-  ozone2$lon.lat[ is.na(ozone2$y[16,]),]
> 
> # 5 draws from process at xp given the data 
> # this is an exact calculation
>  sim.Krig( out,xp, M=5)-> sim.out
> 
> # Compare: stats(sim.out)[3,] to  Exact: predictSE( out, xp)
> # simulations on a grid
> # NOTE this is approximate due to the bilinear interpolation
> # for simulating the unconditional random field. 
> # also more  grids points ( nx and  ny) should be used  
> 
> sim.Krig.approx(out,M=5, nx=20,ny=20)-> sim.out
> 
> # take a look at the ensemble members. 
> 
> predictSurface( out, grid= list( x=sim.out$x, y=sim.out$y))-> look
> 
> zr<- c( 40, 200)
> 
> set.panel( 3,2)
plot window will lay out plots in a 3 by 2 matrix 
> image.plot( look, zlim=zr)
> title("mean surface")
> for ( k in 1:5){
+ image( sim.out$x, sim.out$y, sim.out$z[,,k], col=tim.colors(), zlim =zr)
+ }
> 
> 
> 
> ## Not run: 
> ##D data( ozone2)
> ##D y<- ozone2$y[16,]
> ##D good<- !is.na( y)
> ##D y<-y[good]
> ##D x<- ozone2$lon.lat[good,]
> ##D O3.fit<- mKrig( x,y, Covariance="Matern", aRange=.5,smoothness=1.0, lambda= .01 )
> ##D set.seed(122)
> ##D O3.sim<- sim.mKrig.approx( O3.fit, nx=100, ny=100, gridRefinement=3, M=5 )
> ##D set.panel(3,2)
> ##D surface( O3.fit)
> ##D for ( k in 1:5){
> ##D image.plot( as.surface( O3.sim$predictionPoints, O3.sim$Ensemble[,k]) )
> ##D }
> ##D # conditional simulation at missing data
> ##D xMissing<- ozone2$lon.lat[!good,]
> ##D O3.sim2<- sim.mKrig.approx( O3.fit, xMissing, nx=80, ny=80,
> ##D                             gridRefinement=3, M=4 )
> ## End(Not run)
> ## Not run: 
> ##D #An example for fastTps:
> ##D   data(ozone2)
> ##D   y<- ozone2$y[16,]
> ##D   good<- !is.na( y)
> ##D   y<-y[good]
> ##D   x<- ozone2$lon.lat[good,]
> ##D   O3Obj<- fastTps( x,y, aRange=1.5 )
> ##D # creating a quick grid list based on ranges of locations
> ##D   grid.list<- fields.x.to.grid( O3Obj$x, nx=100, ny=100)
> ##D # controlling the grids
> ##D   xR<- range( x[,1], na.rm=TRUE)
> ##D   yR<- range( x[,2], na.rm=TRUE)
> ##D   simulationGridList<- list( x= seq(xR[1],xR[2],,400),
> ##D          y= seq( yR[1],yR[2], ,400))
> ##D # very fine localized prediction grid
> ##D     O3GridList<- list( x= seq( -90.5,-88.5,,200), y= seq( 38,40,,200))
> ##D     O3Sim<- sim.mKrig.approx( O3Obj, M=5, predictionPointsList=O3GridList,
> ##D                   simulationGridList = simulationGridList)
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("sim.Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("sim.rf")
> ### * sim.rf
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: circulantEmbedding
> ### Title: Efficiently Simulates a Stationary 1 and 2D Gaussian random
> ###   fields
> ### Aliases: sim.rf circulantEmbedding circulantEmbeddingSetup
> ### Keywords: spatial
> 
> ### ** Examples
> 
> 
> #Simulate a Gaussian random field with an exponential covariance function,  
> #range parameter = 2.0 and the domain is  [0,5]X [0,5] evaluating the 
> #field at a 100X100 grid.  
>   grid<- list( x= seq( 0,5,,100), y= seq(0,5,,100)) 
>   obj<- circulantEmbeddingSetup( grid, Covariance="Exponential", aRange=.5)
>   set.seed( 223)
>   look<-  circulantEmbedding( obj)
> # Now simulate another ... 
>   look2<- circulantEmbedding( obj)
> # take a look at first two  
>  set.panel(2,1)
plot window will lay out plots in a 2 by 1 matrix 
>  image.plot( grid[[1]], grid[[2]], look) 
>  title("simulated gaussian fields")
>  image.plot( grid[[1]], grid[[2]], look2) 
>  title("another realization ...")
>  
> # Suppose one requires an exponential, range = 2
> # but marginal variance = 10 ( sigma in fields notation)
> look3<- sqrt( 10)*circulantEmbedding( obj)
> 
> ## Not run: 
> ##D # an interesting 3D field
> ##D 
> ##D grid<- list(  1:40,  1:40, 1:16  )
> ##D 
> ##D obj<- circulantEmbeddingSetup( grid,
> ##D                          cov.args=list( Covariance="Matern", aRange=2, smoothness=1.0)
> ##D                          )
> ##D # NOTE: choice of aRange is close to giving a negative weight array
> ##D set.seed( 122)
> ##D look<- circulantEmbedding( obj )
> ##D # look at slices in the 3rd dimension 
> ##D set.panel( 4,4)
> ##D zr<- range( look)
> ##D par( mar=c(1,1,0,0))
> ##D for(  k in 1:16){
> ##D image( grid[[1]], grid[[2]], look[,,k], zlim= zr, col=tim.colors(256),
> ##D        axes=FALSE, xlab="", ylab="")
> ##D }
> ##D 
> ##D 
> ## End(Not run)
> 
> 
> # same as first example using the older sim.rf
> 
> grid<- list( x= seq( 0,10,length.out=100) , y= seq( 0,10,length.out=100) )
> obj<-Exp.image.cov( grid=grid, aRange=.75, setup=TRUE)
> set.seed( 223)
> look<- sim.rf( obj)
> # Now simulate another ... 
> look2<- sim.rf( obj)
> 
>  
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("sim.rf", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("smooth.2d")
> ### * smooth.2d
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: smooth.2d
> ### Title: Kernel smoother for irregular 2-d data
> ### Aliases: smooth.2d
> ### Keywords: smooth
> 
> ### ** Examples
> 
> # Normal kernel smooth of the precip data with bandwidth of .5 ( degree) 
> #  
> look<- smooth.2d( RMprecip$y,  x=RMprecip$x, aRange=.25)
> 
> # finer resolution used in computing the smooth 
> look3<-smooth.2d( RMprecip$y, x=RMprecip$x, aRange=.25, nrow=256, 
+ ncol=256,Nwidth=32,
+ Mwidth=32) 
> # if the width arguments were omitted the padding would create a  
> # 512X 512 matrix with the data filled in the upper 256X256 part. 
> # with a bandwidth of .25 degrees the normal kernel is essentially zero  
> # beyond 32 grid points from its center ( about 6 standard deviations) 
> #
> # take a look:
> 
> #set.panel(2,1)
> #image( look3, zlim=c(-8,12))
> #points( RMprecip$x, pch=".")  
> #image( look, zlim =c(-8,12))
> #points( RMprecip$x, pch=".")  
> 
> 
> # bandwidth changed to .25, exponential kernel   
> look2<- smooth.2d( RMprecip$y, x=RMprecip$x, cov.function=Exp.cov,aRange=.25)
> # 
> 
> 
>  
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("smooth.2d", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("spatialProcess")
> ### * spatialProcess
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: spatialProcess
> ### Title: Estimates a spatial process model.
> ### Aliases: spatialProcess spatialProcessSetDefaults plot.spatialProcess
> ###   print.spatialProcess print.spatialProcessSummary
> ###   summary.spatialProcess profileMLE confidenceIntervalMLE
> ### Keywords: spatial
> 
> ### ** Examples
> 
> data( ozone2)
> # x is a two column matrix where each row is a location in lon/lat 
> # coordinates
>   x<- ozone2$lon.lat
> # y is a vector of ozone measurements at day 16. Note some missing values. 
>   y<- ozone2$y[16,]
>   
> # artifically reduce size of data for a quick example to pass CRAN ...
>   x<- x[1:75,]
>   y<- y[1:75]
>   
> # lots of default choices  made here -- see gridN to increase 
> # the number of points in grid searches for MLEs
> # without specifying  lambda or aRange both are found in a robust 
> # way uses grid searches 
> # profiling over lambda and aRange  is not reuqired but completes the full 
> # example.  Omit this for a faster computation.
> 
>   obj<- spatialProcess( x, y, profileLambda=TRUE, profileARange=TRUE)
>   
> # summary of model
>   summary( obj)
CALL:
spatialProcess(x = x, y = y, profileLambda = TRUE, profileARange = TRUE)

 SUMMARY OF MODEL FIT:
                                                             
 Number of Observations:                    74               
 Degree of polynomial in fixed part:        1                
 Total number of parameters in fixed part:  3                
 tau  Nugget stan. dev:                     10.22            
 sigma Process variance:                    21.27            
 lambda   tau^2/sigma^2:                    0.2311           
 aRange parameter (in units of distance):   1.115            
 Approx.  degrees of freedom for curve      27.88            
    Standard Error of df estimate:          1.501            
 log Likelihood:                            -305.056427482396
 log Likelihood REML:                       -309.414430504448

 ESTIMATED COEFFICIENTS FOR FIXED PART:

   estimate      SE  pValue
d1  345.800 272.500 0.20440
d2    7.027   3.044 0.02099
d3    8.484   4.636 0.06723

 COVARIANCE MODEL: stationary.cov
  Covariance function:  Matern
   Non-default covariance arguments and their values 
Covariance :
[1] "Matern"
smoothness :
[1] 1
aRange :
[1] 1.115425
onlyUpper :
[1] FALSE
distMat :
[1] NA
Nonzero entries in covariance matrix  5476

SUMMARY FROM Max. Likelihood ESTIMATION:
Parameters found from optim: 
   lambda    aRange 
0.2311332 1.1154248 
Approx. confidence intervals for MLE(s) 
         lower95%  upper95%
lambda 0.08870229 0.6022682
aRange 0.48240121 2.5791239

 Note: MLEs for  tau and sigma found analytically from lambda

Summary from estimation:
lnProfileLike.FULL lnProfileREML.FULL             lambda                tau 
      -305.0564275       -309.4144305          0.2311332         10.2249266 
            sigma2             aRange             eff.df                GCV 
       452.3327549          1.1154248         27.8802229        172.9769500 
>   
> # diagnostic plots
>   set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
>   plot(obj)
[1] TRUE
>   
> # plot 1 data vs. predicted values
> # plot 2 residuals vs. predicted
> # plot 3 criteria to select the smoothing
> #        parameter lambda = tau^2 / sigma
> #        the x axis has log10 lambda
> #        Note that here the GCV function is minimized
> #        while the log profile likelihood  is maximzed. 
> # plot 4 the log profile likelihood used to 
> #        determine range parameter aRange. 
> #
> 
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> # predictions on a grid 
> surface( obj, xlab="longitude", ylab="latitude")
> US( add=TRUE, col="grey", lwd=2)
> title("Predicted ozone (in PPB)  June 18, 1987 ")  
> #(see also predictSurface for more control on evaluation grid, predicting
> # outside convex hull of the data. and plotting)
> 
> # prediction standard errors, note two steps now to generate
> # and then plot surface
> look<- predictSurfaceSE( obj)
> surface( look, xlab="longitude", ylab="latitude")
> points( x, col="magenta")
> title("prediction standard errors (PPB)")
> 
> # here  is a sanity check -- call spatialProcess with the MLEs found
> # above, better get the same predictions!
> 
> objTest<- spatialProcess( x, y,
+                           lambda=obj$MLESummary["lambda"],
+                           aRange=obj$MLESummary["aRange"]
+                           )
>                           
> test.for.zero(objTest$fitted.values, obj$fitted.values, 
+             tag="sanity check" )
Testing:  sanity check
PASSED test at tolerance  1e-08
> 
> ## Not run: 
> ##D # working with covariates and filling in missing station data
> ##D # using an ensemble method
> ##D # see the example under  help(sim.spatialProcess) to see how to 
> ##D # handle a conditional simulation on a grid of predictions with 
> ##D # covariates. 
> ##D data(COmonthlyMet)
> ##D   fit1E<- spatialProcess(CO.loc,CO.tmin.MAM.climate, Z=CO.elev, 
> ##D                                gridARange= seq(.25, 2.0, length.out=10)
> ##D 			       )
> ##D   set.panel( 2,2)                             
> ##D   plot( fit1E)
> ##D   
> ##D # conditional simulation at missing data
> ##D   notThere<- is.na(CO.tmin.MAM.climate )
> ##D   xp <- CO.loc[notThere,]
> ##D   Zp <- CO.elev[notThere]
> ##D   infill<- sim.spatialProcess( fit1E, xp=xp,
> ##D                       Z= Zp, M= 10)
> ##D #  
> ##D # interpretation is that these infilled values are all equally plausible 
> ##D # given the observations and also given the estimated covariance model
> ##D #  
> ##D # EXTRA CREDIT:  standardize the infilled values to have
> ##D # conditional mean and variance from the exact computations
> ##D #  e.g. predict( fit1E, xp=CO.loc[!good,],  Z= CO.elev[!good])
> ##D #  and  predictSE(fit1E, xp=CO.loc[!good,],  Z= CO.elev[!good])  
> ##D # with these standardization one would still preserve the correlations
> ##D # among the infilled values that is also important for considering them as a
> ##D # multivariate prediction.
> ##D # conditional simulation on a grid but not using the covariate of elevation
> ##D  fit2<- spatialProcess(CO.loc,CO.tmin.MAM.climate,
> ##D                         gridARange= seq(.25, 2.0, length.out=10)
> ##D                        )
> ##D # note larger range parameter
> ##D # create 2500 grid points using a handy fields function
> ##D gridList <- fields.x.to.grid( fit2$x, nx=50,ny=50)
> ##D xGrid<- make.surface.grid( gridList)
> ##D ensemble<- sim.spatialProcess( fit2, xp=xGrid, M= 5)
> ##D # this is an "n^3" computation so increasing the grid size 
> ##D # can slow things down for computation 
> ##D image.plot( as.surface( xGrid, ensemble[1,]))
> ##D set.panel()
> ## End(Not run)
> 
> ## Not run: 
> ##D ## changing the covariance model.
> ##D 
> ##D data(ozone2)
> ##D 
> ##D   x<- ozone2$lon.lat
> ##D   y<- ozone2$y[16,]
> ##D   
> ##D # a comparison to using an exponential and Wendland covariance function
> ##D # and great circle distance -- just to make range easier to interpret.
> ##D   obj <- spatialProcess( x, y,
> ##D                               Distance = "rdist.earth")
> ##D 	obj2<- spatialProcess( x, y,
> ##D 	        cov.args = list(Covariance = "Exponential"), 
> ##D                               Distance = "rdist.earth" )
> ##D 	obj3<- spatialProcess( x, y,
> ##D 	        cov.args = list(Covariance = "Wendland",
> ##D 	                        dimension  = 2,
> ##D 	                                 k = 2),
> ##D 	                          Distance = "rdist.earth")
> ##D # obj2 could be also be fit using the argument:
> ##D #   cov.args = list(Covariance = "Matern", smoothness=.5)
> ##D #	                          
> ##D # Note very different range parameters - BTW these are in miles
> ##D # but similar nugget variances. 
> ##D  rbind( Whittle= obj$summary,
> ##D  Exp= obj2$summary,
> ##D Wendland= obj3$summary
> ##D )
> ##D 
> ##D # since the exponential is Matern with smoothness == .5 the first two
> ##D # fits can be compared in terms of their likelihoods
> ##D # the ln likelihood   value is slightly higher for obj verses obj2 (-613.9 >  -614.9)
> ##D # these are the _negative_ log  likelihoods so suggests a preference for the
> ##D # smoothness = 1.0 (Whittle)  model 
> ##D # 
> ##D # does it really matter in terms of spatial prediction?
> ##D set.panel( 3,1)
> ##D surface( obj)
> ##D US( add=TRUE)
> ##D title("Matern sm= 1.0")
> ##D surface( obj2)
> ##D US( add=TRUE)
> ##D title("Matern sm= .5")
> ##D surface( obj3)
> ##D US( add=TRUE)
> ##D title("Wendland k =2")
> ##D # prediction standard errors
> ##D # these take a while because prediction errors are based 
> ##D # directly on the Kriging weight matrix
> ##D # see mKrig for an alternative.
> ##D set.panel( 2,1)
> ##D out.p<- predictSurfaceSE( obj, nx=40,ny=40)
> ##D surface( out.p)
> ##D US( add=TRUE)
> ##D title("Matern sm= 1.0")
> ##D points( x, col="magenta")
> ##D #
> ##D out.p<- predictSurfaceSE( obj, nx=40,ny=40)
> ##D surface( out.p)
> ##D US( add=TRUE)
> ##D points( x, col="magenta")
> ##D title("Matern sm= .5")
> ##D set.panel(1,1)
> ## End(Not run)
> 
> 
> ## Not run: 
> ##D ### comparison with GeoR
> ##D data(ozone2)
> ##D     x<- ozone2$lon.lat
> ##D     y<- ozone2$y[16,]
> ##D     good<-!is.na(y)
> ##D     x1<- x[good,]
> ##D     y1<- y[good]
> ##D     
> ##D # spatialProcess takes longer because of grid search on aRange.    
> ##D     obj<- spatialProcess( x1, y1,
> ##D                           mKrig.args = list(m=1),
> ##D   			                  smoothness = .5)
> ##D     library( geoR)
> ##D     ml.n <- likfit(coords= x1, data=y1, ini = c(570, 3), nug = 50)
> ##D     # compare to 
> ##D     stuffFields<- obj$MLESummary[c("lnProfileLike.FULL",
> ##D                                    "aRange","tau","sigma2")]
> ##D     stuffGeoR<- c( ml.n$loglik, ml.n$phi, sqrt(ml.n$nugget),
> ##D                    ml.n$sigmasq) 
> ##D     test.for.zero(  max(stuffFields/stuffGeoR), 1, tol=.004)
> ##D    
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("spatialProcess", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("splint")
> ### * splint
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: splint
> ### Title: Cubic spline interpolation
> ### Aliases: splint
> ### Keywords: smooth
> 
> ### ** Examples
> 
> x<- seq( 0, 120,,200)
> 
> # an interpolation
> splint(rat.diet$t, rat.diet$trt,x )-> y
> 
> plot( rat.diet$t, rat.diet$trt)
> lines( x,y)
> #( this is weird and not appropriate!)
> 
> # the following two smooths should be the same
> 
> splint( rat.diet$t, rat.diet$con,x, df= 7)-> y1
> 
> # sreg function has more flexibility than splint but will
> # be slower for larger data sets. 
> 
> sreg( rat.diet$t, rat.diet$con, df= 7)-> obj
> predict(obj, x)-> y2 
> 
> # in fact predict.sreg interpolates the predicted values using splint!
> 
> # the two predicted lines (should) coincide
> lines( x,y1, col="red",lwd=2)
> lines(x,y2, col="blue", lty=2,lwd=2)
>  
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("splint", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("sreg")
> ### * sreg
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: sreg
> ### Title: Cubic smoothing spline regression
> ### Aliases: sreg predict.sreg
> ### Keywords: smooth
> 
> ### ** Examples
> 
> # fit a GCV spline to  
> # control group of rats.  
> fit<- sreg(rat.diet$t,rat.diet$con)
> summary( fit)
CALL:
sreg(x = rat.diet$t, y = rat.diet$con)
                                          
 Number of Observations:             39   
 Number of unique points:            39   
 Eff. degrees of freedom for spline: 7.6  
 Residual degrees of freedom:        31.4 
 GCV est. tau                        1.581
 lambda                              10.42

RESIDUAL SUMMARY:
     min    1st Q   median    3rd Q      max 
-4.53200 -0.46270 -0.07006  0.61350  3.55300 

DETAILS ON SMOOTHING PARAMETER:
 Method used:      Cost: 
   lambda       trA       GCV   GCV.one GCV.model    tauHat 
   10.423     7.550     3.098     3.098        NA     1.581 

 Summary of estimates for lambda
        lambda  trA   GCV tauHat converge
GCV      10.42 7.55 3.098  1.581        4
GCV.one  10.42 7.55 3.098  1.581        4
> 
> set.panel(2,2)
plot window will lay out plots in a 2 by 2 matrix 
> plot(fit)                       # four diagnostic plots of  fit 
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> 
> predict( fit) # predicted values at data points 
 [1] 19.84457 19.86637 19.91489 20.10735 20.20748 20.49242 21.43099 21.75381
 [9] 22.47953 24.00058 24.34566 24.94673 25.81778 25.99919 26.34511 26.95274
[17] 27.08416 27.31120 27.58160 27.61676 27.66454 27.76603 27.79317 27.84516
[25] 27.93358 27.95091 27.96834 27.90773 27.88407 27.78388 27.78546 27.82238
[33] 27.85489 27.90905 27.96906 28.04095 28.05798 28.02834 27.82057
> 
> xg<- seq(0,110,,50) 
> sm<-predict( fit, xg) # spline fit at 50 equally spaced points 
> der.sm<- predict( fit, xg, deriv=1) # derivative of spline fit 
> set.panel( 2,1) 
plot window will lay out plots in a 2 by 1 matrix 
> plot( fit$x, fit$y) # the data 
> lines( xg, sm) # the spline 
> plot( xg,der.sm, type="l") # plot of estimated derivative 
> set.panel() # reset panel to 1 plot
plot window will lay out plots in a 1 by 1 matrix 
> 
> 
> # the same fit using  the thin plate spline numerical algorithms 
> # sreg does not scale the obs so instruct Tps not to sacel either
> # this will make lambda comparable within factor of n. 
> 
>    fit.tps<-Tps( rat.diet$t,rat.diet$con, scale="unscaled")
>    summary( fit.tps) 
CALL:
Tps(x = rat.diet$t, Y = rat.diet$con, scale.type = "unscaled")
                                                
 Number of Observations:                39      
 Number of unique points:               39      
 Number of parameters in the null space 2       
 Parameters for fixed spatial drift     2       
 Effective degrees of freedom:          7.6     
 Residual degrees of freedom:           31.4    
 MLE tau                                1.482   
 GCV tau                                1.58    
 MLE sigma                              0.005488
 Scale passed for covariance (sigma)    <NA>    
 Scale passed for nugget (tau^2)        <NA>    
 Smoothing parameter lambda             400.4   

Residual Summary:
     min    1st Q   median    3rd Q      max 
-4.52600 -0.46540 -0.07029  0.61010  3.54900 

Covariance Model: Rad.cov
  Names of non-default covariance arguments: 
       p

DETAILS ON SMOOTHING PARAMETER:
 Method used:   GCV    Cost:  1
   lambda       trA       GCV   GCV.one GCV.model    tauHat 
  400.400     7.575     3.098     3.098        NA     1.580 

 Summary of all estimates found for lambda
           lambda   trA   GCV tauHat -lnLike Prof converge
GCV         400.4 7.575 3.098  1.580        75.54        1
GCV.model      NA    NA    NA     NA           NA       NA
GCV.one     400.4 7.575 3.098  1.580           NA        1
RMSE           NA    NA    NA     NA           NA       NA
pure error     NA    NA    NA     NA           NA       NA
REML       3950.0 4.714 3.110  1.654        74.02        6
> 
> # compare sreg and Tps results to show the adjustment to lambda.
> 
>    predict( fit)-> look
>    predict( fit.tps, lambda=fit$lambda*fit$N)-> look2
>    test.for.zero( look, look2) # silence means it checks to 1e-8
PASSED test at tolerance  1e-08
> 
> # finding approximate standard errors at observations
> 
> SE<- fit$tauHat.GCV*sqrt(fit$diagA)
> 
> # compare to predictSE( fit.tps) differences are due to 
> # slightly different lambda values and using tauHat.MLE instad of tauHat.GCV
> #
> 
> # 95% pointwise prediction intervals
> Zvalue<-  qnorm(.0975)
> upper<- fit$fitted.values + Zvalue* SE
> lower<- fit$fitted.values - Zvalue* SE
> #
> # conservative, simultaneous Bonferroni bounds
> #
> ZBvalue<-  qnorm(1- .025/fit$N)
> upperB<- fit$fitted.values + ZBvalue* SE
> lowerB<- fit$fitted.values - ZBvalue* SE
> #
> # take a look
> 
> plot( fit$x, fit$y, type="n")
> envelopePlot(fit$x, lowerB,fit$x, upperB, col = "grey90",
+              lineCol="grey")
> envelopePlot(fit$x, lower,fit$x, upper, lineCol="grey")
> lines( fit$predicted, col="red",lwd=2)
> points( fit$x, fit$y,pch=16)
> 
> title( "95 pct pointwise  and simultaneous intervals")
> 
> # or try the more visually  honest not connecting points
> plot( fit$x, fit$y, type="n")
> segments(  fit$x, lowerB, fit$x, upperB, col="grey",lwd=3)
> segments(  fit$x, lower, fit$x, upper, col="thistle3", lwd=6)
> lines( fit$predicted, lwd=2,col="red")
> points( fit$x, fit$y,pch=16)
> title( "95 pct pointwise  and simultaneous intervals")
> 
> set.panel( 1,1)
plot window will lay out plots in a 1 by 1 matrix 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("sreg", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("stats")
> ### * stats
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: stats
> ### Title: Calculate summary statistics
> ### Aliases: stats
> ### Keywords: univar
> 
> ### ** Examples
> 
> #Statistics for 8 normal random samples: 
> zork<- matrix( rnorm(200), ncol=8) 
> stats(zork) 
                     [,1]        [,2]       [,3]        [,4]       [,5]
N              25.0000000 25.00000000 25.0000000 25.00000000 25.0000000
mean            0.1686652  0.03223135  0.1654091  0.06924384  0.1047730
Std.Dev.        0.9501080  0.70628065  1.1051952  0.83071749  0.7834642
min            -2.2146999 -1.47075238 -1.8049586 -1.52356680 -0.9109216
Q1             -0.3053884 -0.39428995 -0.7099464 -0.54252003 -0.4616447
median          0.3898432 -0.05931340  0.1532533  0.07434132 -0.1795565
Q3              0.7821363  0.55666320  0.6107264  0.59394619  0.4941883
max             1.5952808  1.35867955  2.4016178  1.58683345  1.7672873
missing values  0.0000000  0.00000000  0.0000000  0.00000000  0.0000000
                      [,6]        [,7]        [,8]
N              25.00000000 25.00000000 25.00000000
mean           -0.40974386  0.08186525  0.07187332
Std.Dev.        0.94926150  1.04846008  0.98889447
min            -1.91435943 -1.48746031 -1.46725003
Q1             -1.11592011 -0.61924305 -0.76608200
median         -0.46353040 -0.07715294 -0.03472603
Q3              0.01739562  0.45699881  0.83037317
max             2.08716655  2.30797840  2.07524501
missing values  0.00000000  0.00000000  0.00000000
> 
> zork<- rnorm( 200)
> id<- sample( 1:8, 200, replace=TRUE)
> stats( zork, by=id)
                        3          1           4           8          5
N              21.0000000 27.0000000 30.00000000 23.00000000 26.0000000
mean            0.1720507  0.1743786 -0.02559066  0.01306515  0.2261141
Std.Dev.        0.9837610  0.9936160  1.08184131  0.88428089  1.0623516
min            -2.2648894 -1.8697888 -2.28912398 -1.36329126 -2.2852355
Q1             -0.2556707 -0.3604255 -0.58625948 -0.66838734 -0.1857888
median          0.3230065  0.2441649 -0.04763742 -0.14587563  0.1753231
Q3              0.7625865  0.6457245  0.69098023  0.78600694  1.1325885
max             1.7196273  2.6491669  1.77842929  1.76355200  1.9713374
missing values  0.0000000  0.0000000  0.00000000  0.00000000  0.0000000
                        7          2           6
N              21.0000000 25.0000000 27.00000000
mean            0.1072286 -0.1731138 -0.13072025
Std.Dev.        1.0066706  1.0180224  1.07133876
min            -1.9189098 -2.8889207 -2.59232767
Q1             -0.4299788 -0.6404817 -0.83746911
median          0.1704895 -0.2894994  0.08296573
Q3              0.5705076  0.4820295  0.57034576
max             2.4976616  1.5778918  1.59291375
missing values  0.0000000  0.0000000  0.00000000
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("stats", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("stats.bin")
> ### * stats.bin
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: stats.bin
> ### Title: Bins data and finds some summary statistics.
> ### Aliases: stats.bin
> ### Keywords: univar
> 
> ### ** Examples
> 
> u<- rnorm( 2000)
> v<- rnorm( 2000)
> x<- u
> y<- .7*u + sqrt(1-.7**2)*v
> 
> look<- stats.bin( x,y) 
> look$stats["Std.Dev.",]
        1         2         3         4         5         6         7         8 
1.1823807 0.7113252 0.7385569 0.7648203 0.7855104 0.6986479 0.7997418 1.1382870 
> 
> data( ozone2)
> # make up a variogram day 16 of Midwest daily ozone ...
> look<- vgram( ozone2$lon.lat, c(ozone2$y[16,]), lon.lat=TRUE)
> 
> # break points
> brk<- seq( 0, 250,,40)
> 
> out<-stats.bin( look$d, look$vgram, breaks=brk)
> # plot bin means, and some quantiles  Q1, median, Q3
> matplot( out$centers, t(out$stats[ c("mean", "median","Q1", "Q3"),]), 
+ type="l",lty=c(1,2,2,2), col=c(3,4,3,4), ylab="ozone PPB")
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("stats.bin", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("summary.Krig")
> ### * summary.Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: summary.Krig
> ### Title: Summary for Krig or spatialProcess estimated models.
> ### Aliases: summary.Krig
> ### Keywords: spatial
> 
> ### ** Examples
> 
> fit<- Krig(ChicagoO3$x, ChicagoO3$y, aRange=100)  
> summary(fit)                            # summary of fit 
CALL:
Krig(x = ChicagoO3$x, Y = ChicagoO3$y, aRange = 100)
                                              
 Number of Observations:                20    
 Number of unique points:               20    
 Number of parameters in the null space 3     
 Parameters for fixed spatial drift     3     
 Effective degrees of freedom:          5.4   
 Residual degrees of freedom:           14.6  
 MLE tau                                3.699 
 GCV tau                                4.012 
 MLE sigma                              20.25 
 Scale passed for covariance (sigma)    <NA>  
 Scale passed for nugget (tau^2)        <NA>  
 Smoothing parameter lambda             0.6755

Residual Summary:
    min   1st Q  median   3rd Q     max 
-6.3880 -1.4160 -0.5873  1.5540  7.5930 

Covariance Model: stationary.cov
  Covariance function is 
  Names of non-default covariance arguments: 
       aRange

DETAILS ON SMOOTHING PARAMETER:
 Method used:   REML    Cost:  1
   lambda       trA       GCV   GCV.one GCV.model    tauHat 
   0.6755    5.4421   22.1105   22.1105        NA    4.0117 

 Summary of all estimates found for lambda
           lambda   trA   GCV tauHat -lnLike Prof converge
GCV        0.9654 4.842 22.02  4.086        49.16        4
GCV.model      NA    NA    NA     NA           NA       NA
GCV.one    0.9654 4.842 22.02  4.086           NA        4
RMSE           NA    NA    NA     NA           NA       NA
pure error     NA    NA    NA     NA           NA       NA
REML       0.6755 5.442 22.11  4.012        49.15        3
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("summary.Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("supportsArg")
> ### * supportsArg
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: supportsArg
> ### Title: Tests if function supports a given argument
> ### Aliases: supportsArg
> 
> ### ** Examples
> 
> ################
> #Test covariance function to see if it supports evaluation of 
> #covariance matrix over upper triangle only
> ################
> 
> supportsArg(Rad.cov, "distMat")
[1] FALSE
> supportsArg(Rad.cov, "onlyUpper")
[1] FALSE
> supportsArg(stationary.cov, "distMat")
[1] TRUE
> supportsArg(stationary.cov, "onlyUpper")
[1] TRUE
> supportsArg(Exp.cov, "distMat")
[1] TRUE
> supportsArg(Exp.cov, "onlyUpper")
[1] TRUE
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("supportsArg", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("surface.Krig")
> ### * surface.Krig
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: surface.Krig
> ### Title: Plots a surface and contours
> ### Aliases: surface.Krig surface.mKrig
> ### Keywords: spatial
> 
> ### ** Examples
> 
> fit<- Krig(ChicagoO3$x,ChicagoO3$y, aRange=30)  # krig fit 
> 
> #Image plot of surface with nice, smooth  contours and shading
> 
> surface(fit, type="C", nx=128, ny=128) 
>  
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("surface.Krig", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("tim.colors")
> ### * tim.colors
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: tim.colors
> ### Title: Some useful color tables for images and tools to handle them.
> ### Aliases: tim.colors larry.colors two.colors designer.colors color.scale
> ###   snow.colors fieldsPlotColors
> ### Keywords: aplot
> 
> ### ** Examples
> 
> 
> tim.colors(10) 
 [1] "#00008F" "#0000FF" "#0070FF" "#00DFFF" "#50FFAF" "#BFFF40" "#FFCF00"
 [8] "#FF6000" "#EF0000" "#800000"
> # returns an array of 10 character strings encoding colors in hex format
> 
> # e.g. (red, green,  blue) values of   (16,255, 239)
> #   translates to "#10FFEF" 
> # rgb( 16/255, 255/255, 239/255, alpha=.5)
> #   gives   "#10FFEF80"  note extra "alpha channel"
> 
> # view some color table choices
> set.panel( 4,1)
plot window will lay out plots in a 4 by 1 matrix 
> fieldsPlotColors( tim.colors())
> title("tim.colors")
> fieldsPlotColors( larry.colors())
> title("larry.colors")
> fieldsPlotColors( two.colors())
> title("two.colors")
> fieldsPlotColors( snow.colors())
> title("snow.colors")
> 
> # a bubble plot with some transparency for overlapping dots
> set.seed(123)
> loc<- matrix( rnorm( 200), 100,2)
> Z<- loc[,1] + loc[,2]
> colorMap<- color.scale( Z, col=tim.colors(10, alpha=.8))
> par( mar=c(5,5,5,5)) # extra room on right for color bar
> plot( loc, col=colorMap, pch=16, cex=2)
> #  add a color scale
>  image.plot(legend.only=TRUE, zlim=range( Z), col=tim.colors(10))
> 
> # using tranparency without alpha the image plot would cover points
> 
> obj<- list( x= 1:8, y=1:10, z= outer( 1:8, 1:10, "+") )
> plot( 1:10,1:10)
> 
> image(obj, col=two.colors(alpha=.5), add=TRUE)
> 
> coltab<- designer.colors(col=c("blue", "grey", "green"),
+                    x= c( 0,.3,1) )
> 		   
> 
> image( obj, col= coltab )
> 
> # peg colors at some desired quantiles  of data.
> # NOTE need 0 and 1 for the color scale to make sense
> x<- quantile( c(obj$z), c(0,.25,.5,.75,1.0) )
> # scale these to [0,1]
> zr<- range( c(obj$z))
> x<- (x-zr[1])/ (zr[2] - zr[1])  
> 
> coltab<- designer.colors(256,rainbow(5), x)
> image( obj$z, col= coltab )
> # see image.plot for adding all kinds of legends
> 
> 
> set.panel()
plot window will lay out plots in a 1 by 1 matrix 
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("tim.colors", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("transformx")
> ### * transformx
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: transformx
> ### Title: Linear transformation
> ### Aliases: transformx
> ### Keywords: manip
> 
> ### ** Examples
> 
> #
> newx<-transformx( ChicagoO3$x, scale.type="range")
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("transformx", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("vgram")
> ### * vgram
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: vgram
> ### Title: Traditional or robust variogram methods for spatial data
> ### Aliases: vgram crossCoVGram plot.vgram boxplotVGram getVGMean
> ### Keywords: spatial
> 
> ### ** Examples
> 
> #
> # compute variogram for the midwest ozone field  day 16
> # (BTW this looks a bit strange!)
> #
> data( ozone2)
> good<- !is.na(ozone2$y[16,])
> x<- ozone2$lon.lat[good,] 
> y<- ozone2$y[16,good]
> 
> look<-vgram( x,y, N=15, lon.lat=TRUE) # locations are in lon/lat so use right
> #distance
> # take a look:
> plot(look, pch=19)
> #lines(look$centers, look$stats["mean",], col=4)
> 
> brk<- seq( 0, 250,, (25 + 1) ) # will give 25 bins.
>  
> ## or some boxplot bin summaries
> 
> boxplotVGram(look, breaks=brk, plot.args=list(type="o"))
> plot(look, add=TRUE, breaks=brk, col=4)
> 
> #
> # compute equivalent covariogram, but leave out the boxplots
> #
> look<-vgram( x,y, N=15, lon.lat=TRUE, type="covariogram")
> plot(look, breaks=brk, col=4)
> 
> #
> # compute equivalent cross-covariogram of the data with itself 
> #(it should look almost exactly the same as the covariogram of 
> #the original data, except with a few more points in the 
> #smallest distance boxplot and points are double counted)
> #
> look = crossCoVGram(x, x, y, y, N=15, lon.lat=TRUE, type="cross-covariogram")
> plot(look, breaks=brk, col=4)
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("vgram", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("vgram.matrix")
> ### * vgram.matrix
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: vgram.matrix
> ### Title: Computes a variogram from an image
> ### Aliases: vgram.matrix plot.vgram.matrix
> ### Keywords: spatial
> 
> ### ** Examples
> 
> # variogram for Lennon image.
> data(lennon)
> out<-vgram.matrix( lennon) 
> 
> plot( out$d, out$vgram, xlab="separation distance", ylab="variogram") 
> # image plot of vgram values by direction.  
> 
> # look at different directions 
> out<-vgram.matrix( lennon, R=8)  
> 
> plot( out$d, out$vgram) 
> # add in different orientations 
> points( out$d.full, out$vgram.full, col="red")
> 
> #image plot of variogram values for different directions. 
> set.panel(1,1)
plot window will lay out plots in a 1 by 1 matrix 
> plot.vgram.matrix( out)
> # John Lennon appears remarkably isotropic!
> 
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("vgram.matrix", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("world")
> ### * world
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: world
> ### Title: Plot of the world
> ### Aliases: world world.color in.land.grid world.land
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D world()
> ##D # add the US
> ##D US( add=TRUE,col="blue")
> ##D 
> ##D world( fill=TRUE) # land filled in black 
> ##D 
> ##D ## Western Europe
> ##D world( xlim=c(-10,18),ylim=c(36,60),fill=TRUE, col="darkgreen",
> ##D border="green1")
> ## End(Not run)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("world", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("xline")
> ### * xline
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: xline
> ### Title: Draw a vertical line
> ### Aliases: xline
> ### Keywords: aplot
> 
> ### ** Examples
> 
> 
> plot( 1:10)
> xline( 6.5, col=2)
>  
> world( col=3) 
> yline( seq( -80,80,10),col=4, lty=2)
> xline( seq( -180,180,10),col=4,lty=2)
> yline( 0, lwd=2, col=4)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("xline", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> cleanEx()
> nameEx("yline")
> ### * yline
> 
> flush(stderr()); flush(stdout())
> 
> base::assign(".ptime", proc.time(), pos = "CheckExEnv")
> ### Name: yline
> ### Title: Draw horizontal lines
> ### Aliases: yline
> ### Keywords: aplot
> 
> ### ** Examples
> 
> world( col=3)
> yline( seq( -80,80,10),col=4, lty=2)
> xline( seq( -180,180,10),col=4,lty=2)
> yline( 0, lwd=2, col=4)
> 
> 
> 
> base::assign(".dptime", (proc.time() - get(".ptime", pos = "CheckExEnv")), pos = "CheckExEnv")
> base::cat("yline", base::get(".format_ptime", pos = 'CheckExEnv')(get(".dptime", pos = "CheckExEnv")), "\n", file=base::get(".ExTimings", pos = 'CheckExEnv'), append=TRUE, sep="\t")
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  18.589 1.462 20.157 0.004 0.005 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
